{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attend Infer Repeat\n",
    "\n",
    "In this tutorial we will implement the \"Attend Infer Repeat\" (AIR) model[1] and apply it to the multi-mnist dataset.\n",
    "\n",
    "<p style='border: 1px solid red; padding: 1em'>\n",
    "Current status:\n",
    "\n",
    "Pyro has everything needed to describe this model and inference strategy, but we have not yet reproduced any results from the paper. We're still working on this, and will update this tutorial and the associated example as we make progress.\n",
    "</p>\n",
    "\n",
    "A [standalone implementation](https://github.com/uber/pyro/blob/dev/examples/air/air.py) of this model is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from collections import namedtuple\n",
    "import pyro\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI\n",
    "from pyro.distributions import Bernoulli, DiagNormal\n",
    "from pyro.util import ng_zeros, ng_ones\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu, sigmoid, softplus, grid_sample, affine_grid\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "To get an idea where we're heading, we first give a brief overview of the model and the approach we'll take to inference.\n",
    "\n",
    "AIR is a generative model of scenes. In this tutorial we will use it to model images from the multi-mnist dataset[1]. Here are some example data points from this data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAACWCAYAAAA7UIUvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCm3/h8QR9SLcMptWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d0126d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn = '../../examples/air/data/multi_mnist_train_uint8.npz'\n",
    "mnist = np.load(fn)['x'].astype(np.float32) / 255.\n",
    "figure(figsize=(12,4))\n",
    "for i, img in enumerate(mnist[9:14]):\n",
    "    subplot(1, 5, i + 1)\n",
    "    imshow(img, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIR decomposes the process of generating a whole image into discrete steps, each of which generates only part of the image. We'll call these parts \"objects\". In the case of AIR applied to the multi-mnist dataset we expect each of these objects to represent a single digit. Each object is represented as a latent \"code\" variable. This code is turned in to pixels by passing it through a neural network, which provides a flexible way of modelling the variability of individual digits. The model also includes uncertainty about the location and size of each object, and the number of objects within each image. Following the paper we'll call an object's location and size its \"pose\".\n",
    "\n",
    "Inference in performed in this model using amortized variational inference. The parameters of the neural network are also optimized during inference. Performing inference in such rich models is always difficult, but the presence of discrete choices (the number of steps in this case) makes inference in this model particularly tricky. For this reason the authors use a technique called data dependent baselines to achieve good performance. This technique can be implemented in Pyro, and we'll see how later in the tutorial.\n",
    "\n",
    "## The Model\n",
    "\n",
    "### Generating a single object\n",
    "\n",
    "Let's look at the model more closely. At the core of the model is the generative process for a single object. Recall that:\n",
    "\n",
    "* At each step a single object is generated.\n",
    "\n",
    "* We maintain uncertainty about the latent code used to generate each\n",
    "  object, as well as its pose.\n",
    "\n",
    "* Each object is generated by passing its latent code through a neural\n",
    "  network.\n",
    "\n",
    "This can be expressed in Pyro like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the neural network. This takes a latent code, z_what, to pixel intensities.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.l1 = nn.Linear(50, 100)\n",
    "        self.l2 = nn.Linear(100, 400)\n",
    "\n",
    "    def forward(self, z_what):\n",
    "        h = relu(self.l1(z_what))\n",
    "        return sigmoid(self.l2(h))\n",
    "\n",
    "decode = Decoder()\n",
    "z_where_params = ()\n",
    "z_what_params = ()\n",
    "\n",
    "def model_step(t):\n",
    "    # Sample object pose. This is a 3-vector representing x,y position and size.\n",
    "    z_where = pyro.sample('z_where_{}'.format(t), DiagNormal(z_where_params))\n",
    "\n",
    "    # Sample object code. Here this is a vector of length 50.\n",
    "    z_what = pyro.sample('z_what_{}'.format(t), DiagNormal(z_what_params))\n",
    "    \n",
    "    # Map code to pixel space using the neural network.\n",
    "    y_att = decode(z_what)\n",
    "\n",
    "    # Position/scale object within larger image.\n",
    "    y = object_to_image(z_where, object_size, x_size, y_att)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that this code is only a sketch of an implementation that we will build up throughout the tutorial and is not executable.)\n",
    "\n",
    "Hopefully the use of `pyro.sample` and PyTorch networks within a model seem familiar at this point. If not you might want to review earlier parts of the tutorial. One thing to note is that we include the current step `t` in the name passed to `pyro.sample` to ensure that names are unique across steps.\n",
    "\n",
    "The `object_to_image` function is specific to this model and warrants further attention. Recall that the neural network (`decode` here) will output a small image, and that we would like to add this to the output image after performing any translation and scaling required to achieve the pose (location and size) described by `z_where`. It's not clear how to do this, and in particular it's not obvious that this can be implemented in a way that preserves the differentiability of our model. However, it turns out we can do this this using a spatial transformer network (STN)[2].\n",
    "\n",
    "Happily for us, PyTorch makes it easy to implement a STN using its [`grid_sample`](http://pytorch.org/docs/master/nn.html#grid-sample) and [`affine_grid`](http://pytorch.org/docs/master/nn.html#affine-grid) functions. `object_to_image` is a simple function that calls these, doing a little extra work to massage `z_where` into the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_z_where(z_where):\n",
    "    # Takes three vectors, and massages them into 2x3 matrices with elements like so:\n",
    "    # [s,x,y] -> [[s,0,x],\n",
    "    #             [0,s,y]]\n",
    "    n = z_where.size(0)\n",
    "    expansion_indices = Variable(torch.LongTensor([1, 0, 2, 0, 1, 3]))\n",
    "    out = torch.cat((ng_zeros([1, 1]).expand(n, 1), z_where), 1)\n",
    "    return torch.index_select(out, 1, expansion_indices).view(n, 2, 3)\n",
    "\n",
    "def object_to_image(z_where, obj):\n",
    "    n = obj.size(0)\n",
    "    theta = expand_z_where(z_where)\n",
    "    grid = affine_grid(theta, torch.Size((n, 1, 50, 50)))\n",
    "    out = grid_sample(obj.view(n, 1, 20, 20), grid)\n",
    "    return out.view(n, 50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A discussion of the details of the STN is beyond the scope of this tutorial. For our purposes however, it suffices to keep in mind that `object_to_image` takes the small image generated by the neural network and places it within a larger image with the desired pose.\n",
    "\n",
    "### Generating an image\n",
    "\n",
    "We have completed the implementation of the model for a single object. Next we need to consider how we use this to generate an entire image. Recall that we would like to maintain uncertainty over the number of steps used to generate each data point. To achieve this, we will sample from a Bernoulli distribution at each step and continue making further steps until we sample a `0`. By doing so, we specify a geometric prior over the number of steps.\n",
    "\n",
    "However, at this point we run into a snag.\n",
    "\n",
    "#### Vectorized mini-batches\n",
    "\n",
    "In our final implementation we would like to generate a mini batch of samples in parallel for efficiency. While Pyro supports vectorized mini batches with `iarange`, it currently assumes that each sample statement within `iarange` samples a mini batch of the same size. This is problematic for us because as we have just described, each sample from our model can make a different number of choices.\n",
    "\n",
    "One simple solution, and the one we'll take here, is to make a fixed number of steps for each sample, but only add the generated object to the final image when we sample a `1` from the Bernoulli distribution.\n",
    "\n",
    "Even though this approach performs redundant computation, the gains from using mini batches are so large that this is still a win overall. (Eventually though, we'd like to be able to express the model in a way\n",
    "that avoids this redundant computation.)\n",
    "\n",
    "Putting this all together, the generative process for a single step looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_step(n, t, prev_x, prev_z_pres):\n",
    "\n",
    "    z_pres = pyro.sample('z_pres_{}'.format(t), Bernoulli(0.5 * prev_z_pres))\n",
    "\n",
    "    # Sample attention window position.\n",
    "    z_where = pyro.sample('z_where_{}'.format(t),\n",
    "                          DiagNormal(\n",
    "                              Variable(torch.Tensor([3, 0, 0])),\n",
    "                              Variable(torch.Tensor([0.1, 1, 1])),\n",
    "                              batch_size=n))\n",
    "\n",
    "    # Sample latent code for contents of the attention window.\n",
    "    z_what = pyro.sample('z_what_{}'.format(t),\n",
    "                         DiagNormal(ng_zeros(50), ng_ones(50), batch_size=n))\n",
    "\n",
    "    # Map latent code to pixel space.\n",
    "    y_att = decode(z_what)\n",
    "\n",
    "    # Position/scale attention window within larger image.\n",
    "    y = object_to_image(z_where, y_att)\n",
    "\n",
    "    # Combine the image generated at this step with the image so far.\n",
    "    # (Note that there's no notion of occlusion here. Overlapping\n",
    "    # objects can create pixel intensities > 1.)\n",
    "    x = prev_x + y * z_pres.view(-1, 1, 1)\n",
    "\n",
    "    return x, z_pres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By iterating this step function we can produce an entire image, composed of multiple objects. Since each image in the multi-mnist dataset contains zero, one or two digits we will allow the model to use up to (and including) three steps. This will allow us to observe whether inference avoids using the unnecessary final step, and to test the model's ability to generalize to images with more digits than are present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prior(n):\n",
    "    x = ng_zeros(n, 50, 50)\n",
    "    z_pres = ng_ones(n, 1)\n",
    "    for t in range(3):\n",
    "        x, z_pres = model_step(n, t, x, z_pres)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now fully specified the prior for our model. Let's visualize some samples to get a feel for this distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAACWCAYAAAA7UIUvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d3acba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = prior(5).data.numpy()\n",
    "figure(figsize=(12,4))\n",
    "for i, s in enumerate(samples):\n",
    "    subplot(1, 5, i + 1)\n",
    "    imshow(s, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the likelihood\n",
    "\n",
    "The last thing we need in order to complete the specification of the model is a likelihood function. Following [1] we will use a Gaussian likelihood with a fixed standard deviation of 0.3. This is straight forward to implement using `pyro.observe`\n",
    "\n",
    "When we later come to perform inference we will find it convenient to package the prior and likelihood into a single function. This is also be a convenient place to introduce `iarange`, which we use to implement data subsampling. (LINK: iarange docs) We'll take the opportunity to do both of these here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    sd = 0.3 * ng_ones(1, 1)\n",
    "    with pyro.iarange('data', data.size(0)) as indices:\n",
    "        batch = data[indices]\n",
    "        x = prior(batch.size(0))\n",
    "        pyro.observe('obs', DiagNormal(x.view(-1, 50 * 50), sd), batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Guide\n",
    "\n",
    "Following the paper we will perform amortized variational inference in this model. (LINK: tutorial) Pyro provides general purpose machinery that implements most of this inference strategy, but as we have seen in earlier tutorials we are required to provide a model specific guide. What we call a guide in Pyro is exactly the entity called the \"inference network\" in the paper.\n",
    "\n",
    "Let's think about how to structure a guide network to take a multi-mnist image as input and produce the parameters of the latent choices in the model as output. Recall that the model is split into a number of discrete steps, it is therefore natural to structure the network as a recurrent network. At each step the recurrent network will generate the parameters for the choices made within the step. The values sampled will be fed back into the recurrent network so that this information can be used when computing the parameters for the next step. (LINK: similar to dmm. link?)\n",
    "\n",
    "As in the model, the core of the guide is the logic for a single step. Here's a sketch of an implementation of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def guide_step_basic(t, data, prev):\n",
    "\n",
    "    # The RNN takes the images and choices from the previous step as input.\n",
    "    rnn_input = torch.cat((data, prev.z_where, prev.z_what, prev.z_pres), 1)\n",
    "    h, c = rnn(rnn_input, (prev.h, prev.c))\n",
    "\n",
    "    # Compute parameters for all choices made this step, by passing\n",
    "    # the RNN hidden start through another neural network.\n",
    "    z_pres_p, z_where_mu, z_where_sigma, z_what_mu, z_what_sigma = predict_basic(h)\n",
    "\n",
    "    z_pres = pyro.sample('z_pres_{}'.format(t),\n",
    "                         Bernoulli(z_pres_p * prev.z_pres))\n",
    "\n",
    "    z_where = pyro.sample('z_where_{}'.format(t),\n",
    "                          DiagNormal(z_where_mu, z_where_sigma))\n",
    "\n",
    "    z_what = pyro.sample('z_what_{}'.format(t),\n",
    "                         DiagNormal(z_what_mu, z_what_sigma))\n",
    "\n",
    "    return # values for next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be a reasonable guide to use with this model, but the paper describes a crucial improvement we can make to the code above. Recall that the guide will output information about an object's pose and its latent code at each step. The improvement we can make is based on the observation that once we have infered the pose of an object, we can do a better job of infering its latent code if we use the pose information to crop the object from the input image, and pass the result (which we'll call a \"window\") through an additional network in order to compute the parameters of the latent code. We'll call this additional network the \"encoder\" below.\n",
    "\n",
    "Here's how we can implement this improved guide, and a fleshed out implementation of the networks involved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = nn.LSTMCell(2554, 256)\n",
    "\n",
    "# Takes pixel intensities of the attention window to parameters (mean,\n",
    "# standard deviation) of the distribution over the latent code,\n",
    "# z_what.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.l1 = nn.Linear(400, 100)\n",
    "        self.l2 = nn.Linear(100, 100)\n",
    "\n",
    "    def forward(self, data):\n",
    "        h = relu(self.l1(data))\n",
    "        a = self.l2(h)\n",
    "        return a[:, 0:50], softplus(a[:, 50:])\n",
    "\n",
    "encode = Encoder()\n",
    "\n",
    "# Takes the guide RNN hidden state to parameters of\n",
    "# the guide distributions over z_where and z_pres.\n",
    "class Predict(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Predict, self).__init__()\n",
    "        self.l = nn.Linear(256, 7)\n",
    "\n",
    "    def forward(self, h):\n",
    "        a = self.l(h)\n",
    "        z_pres_p = sigmoid(a[:, 0:1]) # Squish to [0,1]\n",
    "        z_where_mu = a[:, 1:4]\n",
    "        z_where_sigma = softplus(a[:, 4:]) # Squish to >0\n",
    "        return z_pres_p, z_where_mu, z_where_sigma\n",
    "\n",
    "predict = Predict()\n",
    "\n",
    "def guide_step_improved(t, data, prev):\n",
    "\n",
    "    rnn_input = torch.cat((data, prev.z_where, prev.z_what, prev.z_pres), 1)\n",
    "    h, c = rnn(rnn_input, (prev.h, prev.c))\n",
    "    z_pres_p, z_where_mu, z_where_sigma = predict(h)\n",
    "\n",
    "    z_pres = pyro.sample('z_pres_{}'.format(t),\n",
    "                         Bernoulli(z_pres_p * prev.z_pres))\n",
    "\n",
    "    z_where = pyro.sample('z_where_{}'.format(t),\n",
    "                          DiagNormal(z_where_mu, z_where_sigma))\n",
    "\n",
    "    # New. Crop a small window from the input.\n",
    "    x_att = image_to_object(z_where, data)\n",
    "\n",
    "    # Compute the parameter of the distribution over z_what\n",
    "    # by passing the window through the encoder network.\n",
    "    z_what_mu, z_what_sigma = encode(x_att)\n",
    "\n",
    "    z_what = pyro.sample('z_what_{}'.format(t),\n",
    "                         DiagNormal(z_what_mu, z_what_sigma))\n",
    "\n",
    "    return # values for next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we would like to maintain differentiability of the guide we again use a STN to perform the required \"cropping\". The `image_to_object` function performs the opposite transform to the object_to_image function used in the guide. That is, the former takes a small image and places it on a larger image, and the latter crops a small image from a larger image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def z_where_inv(z_where):\n",
    "    # Take a batch of z_where vectors, and compute their \"inverse\".\n",
    "    # That is, for each row compute:\n",
    "    # [s,x,y] -> [1/s,-x/s,-y/s]\n",
    "    # These are the parameters required to perform the inverse of the\n",
    "    # spatial transform performed in the generative model.\n",
    "    n = z_where.size(0)\n",
    "    out = torch.cat((ng_ones([1, 1]).type_as(z_where).expand(n, 1), -z_where[:, 1:]), 1)\n",
    "    out = out / z_where[:, 0:1]\n",
    "    return out\n",
    "\n",
    "def image_to_object(z_where, image):\n",
    "    n = image.size(0)\n",
    "    theta_inv = expand_z_where(z_where_inv(z_where))\n",
    "    grid = affine_grid(theta_inv, torch.Size((n, 1, 20, 20)))\n",
    "    out = grid_sample(image.view(n, 1, 50, 50), grid)\n",
    "    return out.view(n, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See figure 3b of [1] for a picture of the final guide structure.\n",
    "\n",
    "### Another perspective\n",
    "\n",
    "So far we've considered the model and the guide in isolation, but we gain an interesting perspective if we zoom out and look at the model and guide computation as a whole. Doing so, we see that at each step AIR includes a sub-computation that has the same structure as a Variational Auto-encoder (VAE). (LINK: vae tutorial.)\n",
    "\n",
    "To see this, notice that the guide passes the window through a neural network (the encoder) to generate the parameters of the distribution over a latent code, and the model passes samples from this latent code distribution through another neural network (the decoder) to generate an output window.\n",
    "\n",
    "From this perspective AIR is seen as a sequential variant of the VAE. The act of cropping a small window from the input image serves to restrict the attention of a VAE to a small region of the input image at each step; hence \"Attend, Infer, Repeat\".\n",
    "\n",
    "## Inference\n",
    "\n",
    "As was mentioned in the introduction, successfully performing inference in this model is a challenge. In particular, the presence of discrete choices in the model makes inference trickier than in a model in which all choices are continuous.\n",
    "\n",
    "(LINK: Link to tutorial discussion of combined estimator, trickiness of reinforce.)\n",
    "\n",
    "The underlying problem we face is that the gradient estimates we use in the optimization performed by variational inference have much higher variance in the presence of discrete choices.\n",
    "\n",
    "To bring this variance under control, the paper applies a technique called \"data dependent baselines\" (AKA \"neural baselines\") to the discrete choices in the model. Happily for us Pyro includes support for this technique which means we don't have to do too much work to add this to our model. If you are not already familiar with this idea, you might want to read our introduction before continuing. (LINK: tutorial)\n",
    "\n",
    "### Data dependent baselines\n",
    "\n",
    "Pyro includes support for data dependent baselines. As model authors we only have to implement the neural network, pass it our data as input, and feed its output to `pyro.sample`. Pyro's inference back-end will ensure that the baseline is included in the gradient estimator used for inference, and that the network parameters are updated appropriately.\n",
    "\n",
    "Let's see how we can add data dependent baselines to our AIR implementation. We need a neural network that can output a (scalar) baseline value at each discrete choice in the guide, having received a multi-mnist image and the values sampled by the guide so far as input. Notice that this is very similar to the structure of the guide network, and indeed we will again use a recurrent network.\n",
    "\n",
    "To implement this we will first write a short helper function that implements a single step of the RNN we've just described:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bl_rnn = nn.LSTMCell(2554, 256)\n",
    "bl_predict = nn.Linear(256, 1)\n",
    "\n",
    "# Use an RNN to compute the baseline value. This network takes the\n",
    "# input images and the values samples so far as input.\n",
    "def baseline_step(x, prev):\n",
    "    rnn_input = torch.cat((x,\n",
    "                           prev.z_where.detach(),\n",
    "                           prev.z_what.detach(),\n",
    "                           prev.z_pres.detach()), 1)\n",
    "    bl_h, bl_c = bl_rnn(rnn_input, (prev.bl_h, prev.bl_c))\n",
    "    bl_value = bl_predict(bl_h)\n",
    "    return bl_value, bl_h, bl_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we `detach` values sampled by the guide before passing them to the baseline network. This is important as the baseline network and the guide network are entirely separate networks optimized with different objectives. Without this, gradients would flow from the baseline network in to the guide network. When using data dependent baselines we must do this whenever we feed values sampled by the guide into the baselines network. (If we don't we'll trigger a PyTorch run-time error.)\n",
    "\n",
    "We now have everything we need to complete the implementation of the guide. Our final `guide_step` function will be very similar to `guide_step_improved` introduced above. The only change is that we will call the `baseline_step` helper and pass the baseline value it returns to `pyro.sample`, completing the baseline implementation. We'll also write a `guide` function that will iterate `guide_step` in order to provide a guide for the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GuideState = namedtuple('GuideState', ['h', 'c', 'bl_h', 'bl_c', 'z_pres', 'z_where', 'z_what'])\n",
    "def initial_guide_state(n):\n",
    "    return GuideState(h=ng_zeros(n, 256),\n",
    "                      c=ng_zeros(n, 256),\n",
    "                      bl_h=ng_zeros(n, 256),\n",
    "                      bl_c=ng_zeros(n, 256),\n",
    "                      z_pres=ng_ones(n, 1),\n",
    "                      z_where=ng_zeros(n, 3),\n",
    "                      z_what=ng_zeros(n, 50))\n",
    "\n",
    "def guide_step(t, data, prev):\n",
    "\n",
    "    rnn_input = torch.cat((data, prev.z_where, prev.z_what, prev.z_pres), 1)\n",
    "    h, c = rnn(rnn_input, (prev.h, prev.c))\n",
    "    z_pres_p, z_where_mu, z_where_sigma = predict(h)\n",
    "\n",
    "    # Here we compute the baseline value, and pass it to sample.\n",
    "    baseline_value, bl_h, bl_c = baseline_step(data, prev)\n",
    "    z_pres = pyro.sample('z_pres_{}'.format(t),\n",
    "                         Bernoulli(z_pres_p * prev.z_pres),\n",
    "                         baseline=dict(baseline_value=baseline_value))\n",
    "\n",
    "    z_where = pyro.sample('z_where_{}'.format(t),\n",
    "                          DiagNormal(z_where_mu, z_where_sigma))\n",
    "    \n",
    "    x_att = image_to_object(z_where, data)\n",
    "\n",
    "    z_what_mu, z_what_sigma = encode(x_att)\n",
    "\n",
    "    z_what = pyro.sample('z_what_{}'.format(t),\n",
    "                         DiagNormal(z_what_mu, z_what_sigma))\n",
    "\n",
    "    return GuideState(h=h, c=c, bl_h=bl_h, bl_c=bl_c, z_pres=z_pres, z_where=z_where, z_what=z_what)\n",
    "\n",
    "def guide(data):\n",
    "    with pyro.iarange('data', data.size(0)) as indices:\n",
    "        batch = data[indices]\n",
    "        state = initial_guide_state(batch.size(0))\n",
    "        steps = []\n",
    "        for t in range(3):\n",
    "            state = guide_step(t, data, state)\n",
    "            steps.append(state)\n",
    "        return steps\n",
    "\n",
    "guide((ng_zeros(2, 50 * 50)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it altogether\n",
    "\n",
    "We have now completed the implementation of the model and the guide. As we have already seen in earlier tutorials, we need write only a few more lines of code to begin performing inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, elbo=30156.99\n",
      "i=1, elbo=27305.24\n",
      "i=2, elbo=48764.95\n",
      "i=3, elbo=40512.21\n",
      "i=4, elbo=78596.48\n"
     ]
    }
   ],
   "source": [
    "svi = SVI(model,\n",
    "          guide,\n",
    "          optim.Adam({'lr': 1e-4}),\n",
    "          loss='ELBO',\n",
    "          trace_graph=True)\n",
    "\n",
    "data = Variable(torch.from_numpy(mnist[0:10])).view(-1, 50 * 50)\n",
    "\n",
    "for i in range(5):\n",
    "    loss = svi.step(data)\n",
    "    print('i={}, elbo={:.2f}'.format(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says that we are performing stochastic variational inference (SVI). One key detail here is that we pass the `trace_graph=True` option to `SVI`. This enables a more sophisticated gradient estimator that further reduces the variance of gradient estimates by making use of independence information included in the model. Using the feature is essential to achieving good results in the presence of discrete choices. You can read more about it in (LINK: to appropriate bit of tutorial.)\n",
    "\n",
    "## Improvements\n",
    "\n",
    "Here are a few ways in which the implementation given here might be improved:\n",
    "\n",
    "* It is reported to be useful in practice to use a different learning rate for the baseline network. In [1] a learning rate of `1e-4` was used for the guide network, and a learning rate of `1e-3` was used for the baseline network. This is straight forward to implement in Pyro by tagging modules (LINK: modules tagging docs) associated with the baseline network and passing multiple learning rates to the optimizer. (LINK: full air example.)\n",
    "* Use bigger neural networks.\n",
    "* Make the initial guide state into optimizable parameters.\n",
    "\n",
    "## Results\n",
    "\n",
    "`TODO: Add results`\n",
    "\n",
    "## References\n",
    "\n",
    "[1] `Attend, Infer, Repeat: Fast Scene Understanding with Generative Models`\n",
    "<br />&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "S. M. Ali Eslami and Nicolas Heess and Theophane Weber and Yuval Tassa and Koray Kavukcuoglu and Geoffrey E. Hinton\n",
    "\n",
    "[2] `Spatial Transformer Networks`\n",
    "<br />&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Max Jaderberg and Karen Simonyan and Andrew Zisserman"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
