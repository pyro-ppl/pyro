{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom SVI Objectives\n",
    "\n",
    "Pyro provides support for various optimization-based approaches to Bayesian inference, with `Trace_ELBO` serving as the basic implementation of SVI (stochastic variational inference).\n",
    "See the [docs](http://docs.pyro.ai/en/0.2.1-release/inference_algos.html#module-pyro.infer.svi) for more information on the various SVI implementations and SVI \n",
    "tutorials [I](http://pyro.ai/examples/svi_part_i.html), \n",
    "[II](http://pyro.ai/examples/svi_part_ii.html), \n",
    "and [III](http://pyro.ai/examples/svi_part_iii.html) for background on SVI.\n",
    "\n",
    "In this tutorial we show how advanced users can modify and/or augment the variational\n",
    "objectives (alternatively: loss functions) provided by Pyro to support special use cases.\n",
    "\n",
    "## Basic SVI Usage\n",
    "\n",
    "We first review the basic usage pattern of `SVI` objects in Pyro. We assume that the user\n",
    "has defined a `model` and a `guide`.  The user then creates an optimizer and an `SVI` object:\n",
    "\n",
    "```python\n",
    "optimizer = pyro.optim.Adam({\"lr\": 0.001, \"betas\": (0.90, 0.999)})\n",
    "svi = pyro.infer.SVI(model, guide, optimizer, loss=pyro.infer.Trace_ELBO())\n",
    "```\n",
    "\n",
    "Gradient steps can then be taken with a call to `svi.step(...)`.\n",
    "\n",
    "\n",
    "## A Lower-Level Pattern\n",
    "\n",
    "The nice thing about the above pattern is that it allows Pyro to take care of various \n",
    "details for us, for example:\n",
    "\n",
    "- `pyro.optim.Adam` dynamically creates a new `torch.optim.Adam` optimizer whenever a new parameter is encountered \n",
    "- `SVI.step()` zeros gradients between gradient steps\n",
    "\n",
    "If we want more control, we can directly manipulate the differentiable loss method of \n",
    "the various `ELBO` classes. For example, (assuming we know all the parameters in advance) \n",
    "this is equivalent to the previous code snippet:\n",
    "\n",
    "```python\n",
    "# define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(my_parameters, {\"lr\": 0.001, \"betas\": (0.90, 0.999)})\n",
    "loss_fn = pyro.infer.Trace_ELBO.differentiable_loss\n",
    "# compute loss\n",
    "loss = loss_fn(model, guide)\n",
    "loss.backward()\n",
    "# take a step and zero the parameter gradients\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "### Example: Custom Regularizer\n",
    "\n",
    "Suppose I want to add a custom regularization term to my SVI loss. Using the above \n",
    "usage pattern, this is easy to do. First we define our regularizer:\n",
    "\n",
    "```python\n",
    "def my_custom_L2_regularizer(my_parameters):\n",
    "   reg_loss = 0.0\n",
    "   for param in my_parameters:\n",
    "       reg_loss = reg_loss + param.pow(2.0).sum()\n",
    "   return reg_loss  \n",
    "```\n",
    "\n",
    "Then the only change we need to make is:\n",
    "\n",
    "```diff\n",
    "- loss = loss_fn(model, guide)\n",
    "+ loss = loss_fn(model, guide) + my_custom_L2_regularizer(my_parameters)\n",
    "```\n",
    "\n",
    "### Example: Scaling the Loss\n",
    "\n",
    "Depending on the optimization algorithm, the scale of the loss may or not matter. Suppose \n",
    "we want to scale our loss function by the number of datapoints before we differentiate it.\n",
    "This is easily done:\n",
    "\n",
    "```diff\n",
    "- loss = loss_fn(model, guide)\n",
    "+ loss = loss_fn(model, guide) / N_data\n",
    "```\n",
    "\n",
    "Note that in the case of SVI, where each term in the loss function is a log probability \n",
    "from the model or guide, this same effect can be achieved using [`poutine.scale`](http://docs.pyro.ai/en/0.2.1-release/poutine.html#pyro.poutine.scale). For \n",
    "example we can use the `poutine.scale` decorator to scale both the model and guide:\n",
    "\n",
    "```python\n",
    "@poutine.scale(scale=1.0/N_data)\n",
    "def model(...):\n",
    "   pass\n",
    "   \n",
    "@poutine.scale(scale=1.0/N_data)\n",
    "def guide(...):\n",
    "   pass\n",
    "```\n",
    "\n",
    "### Example: Custom ELBO\n",
    "\n",
    "In the previous two examples we bypassed creating a `SVI` object and directly manipulated \n",
    "the differentiable loss function provided by an `ELBO` implementation. Another thing we \n",
    "can do is create custom `ELBO` implementations and pass those into the `SVI` machinery. \n",
    "For example, a simplified version of a `Trace_ELBO` loss function might look as follows:\n",
    "\n",
    "```python\n",
    "# note that simple_elbo takes a model, a guide, and their respective arguments as inputs\n",
    "def simple_elbo(model, guide, *args, **kwargs):\n",
    "    # run the guide and trace its execution\n",
    "    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n",
    "    # run the model and replay it against the samples in the guide\n",
    "    model_trace = poutine.trace(\n",
    "        poutine.replay(model, trace=guide_trace)).get_trace(*args, **kwargs)\n",
    "    # construct the elbo loss function\n",
    "    return -1*(model_trace.log_prob_sum() - guide_trace.log_prob_sum())\n",
    "\n",
    "svi = SVI(model, guide, optim, loss=simple_elbo)\n",
    "```\n",
    "Note that this is basically what the `elbo` implementation in ['mini-pyro'](https://github.com/uber/pyro/blob/dev/pyro/contrib/minipyro.py) looks like."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
