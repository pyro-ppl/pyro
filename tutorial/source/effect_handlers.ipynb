{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Readers are strongly encouraged to review our didactic `minipyro` example first:**\n",
    "\n",
    "**https://github.com/uber/pyro/blob/dev/pyro/contrib/minipyro.py**\n",
    "\n",
    "**This tutorial is an API guide. Readers looking for an overview of Pyro's internals should start there instead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import queue\n",
    "\n",
    "import torch\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "from pyro.poutine.runtime import effectful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Inference in probabilistic programming involves manipulating or transforming probabilistic programs written as generative models. For example, nearly all approximate inference algorithms require computing the unnormalized joint probability of values of latent and observed variables under a generative model.\n",
    "\n",
    "Consider the following example model from the [introductory inference tutorial](http://pyro.ai/examples/intro_part_ii.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(guess):\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0))\n",
    "    return pyro.sample(\"measurement\", dist.Normal(weight, 0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model defines a joint probability distribution over `\"weight\"` and `\"measurement\"`:\n",
    "\n",
    "$${\\sf weight} \\, | \\, {\\sf guess} \\sim \\cal {\\sf Normal}({\\sf guess}, 1) $$\n",
    "$${\\sf measurement} \\, | \\, {\\sf guess}, {\\sf weight} \\sim {\\sf Normal}({\\sf weight}, 0.75)$$\n",
    "\n",
    "If we had access to the inputs and outputs of each `pyro.sample` site, we could compute their log-joint:\n",
    "```python\n",
    "logp = dist.Normal(guess, 1.0).log_prob(weight).sum() + dist.Normal(weight, 0.75).log_prob(measurement).sum()\n",
    "```\n",
    "However, the way we wrote `scale` above does seem to expose these intermediate distribution objects, and rewriting it to return them would be intrusive and would violate the separation of concerns between models and inference algorithms that a probabilistic programming language like Pyro is designed to enforce.\n",
    "\n",
    "To resolve this conflict and facilitate inference algorithm development, Pyro exposes [Poutine](http://docs.pyro.ai/en/dev/poutine.html), a library of [*effect handlers*](http://homepages.inf.ed.ac.uk/slindley/papers/handlers.pdf), or composable building blocks for examining and modifying the behavior of Pyro programs. Most of Pyro's internals are implemented on top of Poutine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first look at Poutine: a library of effect handlers in Pyro\n",
    "\n",
    "Effect handlers add *side effects* to the behavior of `pyro.sample` and other operations. Before reviewing more definitions, let's look at a first example that addresses the problem above: we can compose two existing effect handlers, `poutine.condition` (which sets output values of `pyro.sample` statements) and `poutine.trace` (which records the inputs, distributions, and outputs of `pyro.sample` statements), to concisely define a new effect handler that computes the log-joint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.0203)\n"
     ]
    }
   ],
   "source": [
    "def make_log_joint(model):\n",
    "    def _log_joint(data, *args, **kwargs):\n",
    "        conditioned_model = poutine.condition(model, data=data)\n",
    "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
    "        return trace.log_prob_sum()\n",
    "    return _log_joint\n",
    "\n",
    "scale_log_joint = make_log_joint(scale)\n",
    "print(scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That snippet is short, but still somewhat opaque - `poutine.condition`, `poutine.trace`, and `trace.log_prob_sum` are all black boxes.  Let's remove a layer of boilerplate from `make_log_joint`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.0203)\n"
     ]
    }
   ],
   "source": [
    "from pyro.poutine.trace_messenger import TraceMessenger\n",
    "from pyro.poutine.condition_messenger import ConditionMessenger\n",
    "\n",
    "def make_log_joint_2(model):\n",
    "    def _log_joint(data, *args, **kwargs):\n",
    "        with TraceMessenger() as tracer:\n",
    "            with ConditionMessenger(data=data):\n",
    "                model(*args, **kwargs)\n",
    "        \n",
    "        trace = tracer.trace\n",
    "        logp = 0.\n",
    "        for name, node in trace.nodes.items():\n",
    "            if node[\"type\"] == \"sample\":\n",
    "                if node[\"is_observed\"]:\n",
    "                    assert node[\"value\"] is data[name]\n",
    "                logp = logp + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
    "        return logp\n",
    "    return _log_joint\n",
    "\n",
    "scale_log_joint = make_log_joint_2(scale)\n",
    "print(scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes things a little more clear: we can now see that `poutine.trace` and `poutine.condition` are wrappers for context managers that presumably communicate with the model through something inside `pyro.sample`. We can also see that `poutine.trace`  produces a data structure (a [`Trace`](http://docs.pyro.ai/en/dev/poutine.html#trace)) containing a dictionary whose keys are `sample` site names and values are dictionaries containing the distribution (`\"fn\"`) and output (`\"value\"`) at each site, and that the output values at each site are exactly the values specified in `data`.\n",
    "\n",
    "Finally, `TraceMessenger` and `ConditionMessenger` are Pyro effect handlers, or `Messenger`s: stateful context manager objects that are placed on a global stack and pass messages (hence the name) up and down the stack at each effectful operation, like a `pyro.sample` call.  A `Messenger` is placed at the bottom of the stack when its `__enter__` method is called, i.e. when it is used in a \"with\" statement. We'll return to `Messenger` in a couple of sections.\n",
    "\n",
    "For a simplified implementation of this mechanism in only a few lines of code, see [pyro.contrib.minipyro](https://github.com/uber/pyro/blob/dev/pyro/contrib/minipyro.py). For background reading on effect handlers, see the paper [\"Handlers in Action\"](http://homepages.inf.ed.ac.uk/slindley/papers/handlers.pdf) by Ohad Kammar, Sam Lindley, and Nicolas Oury."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing inference algorithms with existing effect handlers\n",
    "\n",
    "It turns out that many inference operations, like our first version of `make_log_joint` above, have strikingly short implementations in terms of existing effect handlers in `pyro.poutine`. For example, here is an implementation of variational inference with a Monte Carlo ELBO that uses `poutine.trace`, `poutine.condition`, and `poutine.replay`.  This is very similar to the simple ELBO in [pyro.contrib.minipyro](https://github.com/uber/pyro/blob/dev/pyro/contrib/minipyro.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_elbo(model, guide, batch, *args, **kwargs):\n",
    "    # assuming batch is a dictionary, we use poutine.condition to fix values of observed variables\n",
    "    conditioned_model = poutine.condition(model, data=batch)\n",
    "    \n",
    "    # we'll approximate the expectation in the ELBO with a single sample:\n",
    "    # first, we run the guide forward unmodified and record values and distributions\n",
    "    # at each sample site using poutine.trace\n",
    "    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n",
    "    \n",
    "    # we use poutine.replay to set the values of latent variables in the model\n",
    "    # to the values sampled above by our guide, and use poutine.trace\n",
    "    # to record the distributions that appear at each sample site in in the model\n",
    "    model_trace = poutine.trace(\n",
    "        poutine.replay(conditioned_model, trace=guide_trace)\n",
    "    ).get_trace(*args, **kwargs)\n",
    "    \n",
    "    elbo = 0.\n",
    "    for name, node in model_trace.nodes.items():\n",
    "        if node[\"type\"] == \"sample\":\n",
    "            elbo = elbo + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
    "            if not node[\"is_observed\"]:\n",
    "                elbo = elbo - guide_trace.nodes[name][\"fn\"].log_prob(node[\"value\"]).sum()\n",
    "    return -elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `poutine.trace` and `poutine.block` to record `pyro.param` calls for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, guide, data):\n",
    "    optimizer = pyro.optim.Adam({})\n",
    "    for batch in data:\n",
    "        # this poutine.trace will record all of the parameters that appear in the model and guide\n",
    "        # during the execution of monte_carlo_elbo\n",
    "        with poutine.trace() as param_capture:\n",
    "            # we use poutine.block here so that only parameters appear in the trace above\n",
    "            with poutine.block(hide_fn=lambda node: node[\"type\"] != \"param\"):\n",
    "                loss = monte_carlo_elbo(model, guide, batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        params = set(node[\"value\"].unconstrained()\n",
    "                     for node in param_capture.trace.nodes.values())\n",
    "        optimizer.step(params)\n",
    "        pyro.infer.util.zero_grads(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a very different inference algorithm--exact inference via enumeration--implemented with `pyro.poutine`.  A complete explanation of this algorithm is beyond the scope of this tutorial and may be found in Chapter 3 of the short online book [Design and Implementation of Probabilistic Programming Languages](http://dippl.org/chapters/03-enumeration.html).  This example uses `poutine.queue`, itself implemented using `poutine.trace`, `poutine.replay`, and `poutine.block`, to enumerate over possible values of all discrete variables in a model and compute a marginal distribution over all possible return values or the possible values at a particular sample site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_discrete_marginal(model, data, site_name=\"_RETURN\"):\n",
    "    \n",
    "    from six.moves import queue  # queue data structures\n",
    "    q = queue.Queue()  # Instantiate a first-in first-out queue\n",
    "    q.put(poutine.Trace())  # seed the queue with an empty trace\n",
    "    \n",
    "    # as before, we fix the values of observed random variables with poutine.condition\n",
    "    # assuming data is a dictionary whose keys are names of sample sites in model\n",
    "    conditioned_model = poutine.condition(model, data=data)\n",
    "    \n",
    "    # we wrap the conditioned model in a poutine.queue,\n",
    "    # which repeatedly pushes and pops partially completed executions from a Queue()\n",
    "    # to perform breadth-first enumeration over the set of values of all discrete sample sites in model\n",
    "    enum_model = poutine.queue(conditioned_model, queue=q)\n",
    "    \n",
    "    # actually perform the enumeration by repeatedly tracing enum_model\n",
    "    # and accumulate samples and trace log-probabilities for postprocessing\n",
    "    samples, log_weights = [], []\n",
    "    while not q.empty():\n",
    "        trace = poutine.trace(enum_model).get_trace()\n",
    "        samples.append(trace.nodes[site_name][\"value\"])\n",
    "        log_weights.append(trace.log_prob_sum())\n",
    "        \n",
    "    # we take the samples and log-joints and turn them into a histogram:\n",
    "    samples = torch.stack(samples, 0)\n",
    "    log_weights = torch.stack(log_weights, 0)\n",
    "    log_weights = log_weights - dist.util.logsumexp(log_weights, dim=0)\n",
    "    return dist.Empirical(samples, log_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that `sequential_discrete_marginal` is very general, but is also quite slow. For high-performance parallel enumeration that applies to a less general class of models, see the enumeration tutorial.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing new effect handlers with the `Messenger` API\n",
    "\n",
    "Although it's easiest to build new effect handlers by composing the existing ones in `pyro.poutine`, implementing a new effect as a `Messenger` is actually fairly straightforward. As a first example, let's implement a version of our log-joint computation that performs the sum while the model is executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.0203)\n",
      "tensor(-3.0203)\n"
     ]
    }
   ],
   "source": [
    "class LogJointMessenger(poutine.messenger.Messenger):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    # __call__ allows Messengers to be used as higher-order functions.\n",
    "    # Messenger already defines __call__, but we re-define it here\n",
    "    # for exposition and to change the return value:\n",
    "    def __call__(self, fn):\n",
    "        def _fn(*args, **kwargs):\n",
    "            with self:\n",
    "                fn(*args, **kwargs)\n",
    "                return self.logp.clone()\n",
    "        return _fn\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.logp = torch.tensor(0.)\n",
    "        return super(LogJointMessenger, self).__enter__()\n",
    "    \n",
    "    # __exit__ takes the same arguments in all Python context managers\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.logp = torch.tensor(0.)\n",
    "        return super(LogJointMessenger, self).__exit__(exc_type, exc_value, traceback)\n",
    "        \n",
    "    def _pyro_sample(self, msg):\n",
    "        assert msg[\"name\"] in self.data\n",
    "        msg[\"value\"] = self.data[msg[\"name\"]]\n",
    "        msg[\"is_observed\"] = True\n",
    "        self.logp = self.logp + (msg[\"scale\"] * msg[\"fn\"].log_prob(msg[\"value\"])).sum()\n",
    "\n",
    "with LogJointMessenger(data={\"measurement\": 9.5, \"weight\": 8.23}) as m:\n",
    "    scale(8.5)\n",
    "    print(m.logp.clone())\n",
    "    \n",
    "scale_log_joint = LogJointMessenger(data={\"measurement\": 9.5, \"weight\": 8.23})(scale)\n",
    "print(scale_log_joint(8.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient bit of boilerplate that allows the use of `LogJointMessenger` as a context manager, decorator, or higher-order function is the following.  Most of the existing effect handlers in `pyro.poutine`, including `poutine.trace` and `poutine.condition` which we used earlier, are `Messenger`s wrapped this way in `pyro.poutine.handlers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.0203)\n"
     ]
    }
   ],
   "source": [
    "def log_joint(model=None, data=None):\n",
    "    msngr = LogJointMessenger(data=data)\n",
    "    return msngr(model) if model is not None else msngr\n",
    "\n",
    "scale_log_joint = log_joint(scale, data={\"measurement\": 9.5, \"weight\": 8.23})\n",
    "print(scale_log_joint(8.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Messenger` API in more detail\n",
    "\n",
    "A generic `Messenger` actually contains two methods that are called once per operation where side effects are performed:\n",
    "1. `_process_message` modifies a message and passes the result to the `Messenger` just above on the stack\n",
    "2. `_postprocess_message` modifies a message and passes the result to the next `Messenger` down on the stack.\n",
    "\n",
    "Although custom `Messenger`s can override these, it's convenient to avoid requiring all effect handlers to be aware of all possible effectful operation types. For this reason, by default `Messenger._process_message` will use `msg[\"type\"]` to dispatch to a corresponding method `Messenger._pyro_<type>`, e.g. `Messenger._pyro_sample` as we wrote in `LogJointMessenger`.  Just as exception handling code ignores unhandled exception types, this allows `Messenger`s to simply pass operations they don't know how to handle up to the next `Messenger` in the stack:\n",
    "```python\n",
    "class Messenger(object):\n",
    "    ...\n",
    "    def _process_message(self, msg):\n",
    "        method_name = \"_pyro_{}\".format(msg[\"type\"])  # e.g. _pyro_sample when msg[\"type\"] == \"sample\"\n",
    "        if hasattr(self, method_name):\n",
    "            getattr(self, method_name)(msg)\n",
    "    ...\n",
    "```\n",
    "`_postprocess_message` is necessary because some effects can only be applied after all other effect handlers have had a chance to update the message once. In the case of `LogJointMessenger`, other effects, like enumeration, may modify a sample site's value or distribution (`msg[\"value\"]` or `msg[\"fn\"]`), so we move the log-probability computation to a new method, `_pyro_post_sample`, which is called by `_postprocess_message` at each `sample` site after all active handlers' `_pyro_sample` methods have been applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.0203)\n"
     ]
    }
   ],
   "source": [
    "class LogJointMessenger2(poutine.messenger.Messenger):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __call__(self, fn):\n",
    "        def _fn(*args, **kwargs):\n",
    "            with self:\n",
    "                fn(*args, **kwargs)\n",
    "                return self.logp.clone()\n",
    "        return _fn\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.logp = torch.tensor(0.)\n",
    "        return super(LogJointMessenger2, self).__enter__()\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.logp = torch.tensor(0.)\n",
    "        return super(LogJointMessenger2, self).__exit__(exc_type, exc_value, traceback)\n",
    "\n",
    "    def _pyro_sample(self, msg):\n",
    "        if msg[\"name\"] in self.data:\n",
    "            msg[\"value\"] = self.data[msg[\"name\"]]\n",
    "            msg[\"done\"] = True\n",
    "            \n",
    "    def _pyro_post_sample(self, msg):\n",
    "        assert msg[\"done\"]  # the \"done\" flag asserts that no more modifications to value and fn will be performed.\n",
    "        self.logp = self.logp + (msg[\"scale\"] * msg[\"fn\"].log_prob(msg[\"value\"])).sum()\n",
    "\n",
    "\n",
    "with LogJointMessenger2(data={\"measurement\": 9.5, \"weight\": 8.23}) as m:\n",
    "    scale(8.5)\n",
    "    print(m.logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside the messages sent by `Messenger`s\n",
    "\n",
    "As the previous two examples suggest, the actual messages passed up and down the stack are dictionaries with a particular set of keys. Consider the following sample statement:\n",
    "```python\n",
    "pyro.sample(\"x\", dist.Bernoulli(0.5), infer={\"enumerate\": \"parallel\"}, obs=None)\n",
    "```\n",
    "This sample statement is converted into an initial message before any effects are applied, and each effect handler's `_process_message` and `_postprocess_message` may update fields in place or add new fields.  We write out the full initial message here for completeness:\n",
    "```python\n",
    "msg = {\n",
    "    # The following fields contain the name, inputs, function, and output of a site.\n",
    "    # These are generally the only fields you'll need to think about.\n",
    "    \"name\": \"x\",\n",
    "    \"fn\": dist.Bernoulli(0.5),\n",
    "    \"value\": None,  # msg[\"value\"] will eventually contain the value returned by pyro.sample\n",
    "    \"is_observed\": False,  # because obs=None by default; only used by sample sites\n",
    "    \"args\": (),  # positional arguments passed to \"fn\" when it is called; usually empty for sample sites\n",
    "    \"kwargs\": {},  # keyword arguments passed to \"fn\" when it is called; usually empty for sample sites\n",
    "    # This field typically contains metadata needed or stored by a particular inference algorithm\n",
    "    \"infer\": {\"enumerate\": \"parallel\"},\n",
    "    # The remaining fields are generally only used by Pyro's internals,\n",
    "    # or for implementing more advanced effects beyond the scope of this tutorial\n",
    "    \"type\": \"sample\",  # label used by Messenger._process_message to dispatch, in this case to _pyro_sample\n",
    "    \"done\": False,\n",
    "    \"stop\": False,\n",
    "    \"scale\": torch.tensor(1.),  # Multiplicative scale factor that can be applied to each site's log_prob\n",
    "    \"mask\": None,\n",
    "    \"continuation\": None,\n",
    "    \"cond_indep_stack\": (),  # Will contain metadata from each pyro.plate enclosing this sample site.\n",
    "}\n",
    "```\n",
    "Note that when we use `poutine.trace` or `TraceMessenger` as in our first two versions of `make_log_joint`, the contents of `msg` are exactly the information stored in the trace for each sample and param site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A less trivial `Messenger` example: lazy evaluation\n",
    "\n",
    "Now that we've learned more about the internals, let's use it to implement a slightly more complicated effect: lazy evaluation. We first define a `LazyValue` class that we will use to build up a computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyValue(object):\n",
    "    def __init__(self, fn, *args, **kwargs):\n",
    "        self._expr = (fn, args, kwargs)\n",
    "        self._value = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"({} {})\".format(str(self._expr[0]), \" \".join(map(str, self._expr[1])))\n",
    "        \n",
    "    def evaluate(self):\n",
    "        if self._value is None:\n",
    "            fn, args, kwargs = self._expr\n",
    "            fn = fn.evaluate() if isinstance(fn, LazyValue) else fn\n",
    "            args = tuple(arg.evaluate() if isinstance(arg, LazyValue) else arg\n",
    "                         for arg in args)\n",
    "            kwargs = {k: v.evaluate() if isinstance(v, LazyValue) else v\n",
    "                      for k, v in kwargs.items()}\n",
    "            self._value = fn(*args, **kwargs)\n",
    "        return self._value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `LazyValue`, implementing lazy evaluation as a `Messenger` compatible with other effect handlers is suprisingly easy. We just make each `msg[\"value\"]` a `LazyValue`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyMessenger(pyro.poutine.messenger.Messenger):\n",
    "    def _process_message(self, msg):\n",
    "        if msg[\"type\"] in (\"apply\", \"sample\") and not msg[\"done\"]:\n",
    "            msg[\"done\"] = True\n",
    "            msg[\"value\"] = LazyValue(msg[\"fn\"], *msg[\"args\"], **msg[\"kwargs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just like `torch.autograd` overloads `torch` tensor operations to record an autograd graph, we need to wrap any operations we'd like to be lazy.  We'll use `pyro.poutine.runtime.effectful` as a decorator to expose these operations to `LazyMessenger`. `effectful` constructs a message much like the one above and passes it up and down the effect handler stack, but allows us to set the type (in this case, to `\"apply\"` instead of `\"sample\"`) so that deterministic operations aren't mistaken for `sample` statements by other effect handlers like `TraceMessenger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@effectful(type=\"apply\")\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "@effectful(type=\"apply\")\n",
    "def mul(x, y):\n",
    "    return x * y\n",
    "\n",
    "@effectful(type=\"apply\")\n",
    "def sigmoid(x):\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "@effectful(type=\"apply\")\n",
    "def normal(loc, scale):\n",
    "    return dist.Normal(loc, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied to another model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<function normal at 0x7f0d3f57b2f0> (<function add at 0x7f0d403f6ea0> (<function mul at 0x7f0d3f57b0d0> ((<function normal at 0x7f0d3f57b2f0> 8.5 1.0) ) 0.8) 1.0) (<function sigmoid at 0x7f0d3f57b1e0> ((<function normal at 0x7f0d3f57b2f0> 0.0 0.25) ))) )\n",
      "tensor(8.0895)\n"
     ]
    }
   ],
   "source": [
    "def biased_scale(guess):\n",
    "    weight = pyro.sample(\"weight\", normal(guess, 1.))\n",
    "    tolerance = pyro.sample(\"tolerance\", normal(0., 0.25))\n",
    "    return pyro.sample(\"measurement\", normal(add(mul(weight, 0.8), 1.), sigmoid(tolerance)))\n",
    "\n",
    "with LazyMessenger():\n",
    "    v = biased_scale(8.5)\n",
    "    print(v)\n",
    "    print(v.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with other effect handlers like `TraceMessenger` and `ConditionMessenger`, with which it freely composes, `LazyMessenger` demonstrates how to use Poutine to quickly and concisely implement state-of-the-art PPL techniques like [delayed sampling with Rao-Blackwellization](https://arxiv.org/abs/1708.07787)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
