{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "Variational autoencoders (VAE) are generative networks that incorporate neural networks and latent variable models.  It consists of an encoder and a decoder that seeks to reconstruct the input from a compressed latent represenation of the input.  It is a great example of how we can use variational inference to solve machine learning problems. We demonstrate how to easily implement a VAE in Pyro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import the packages and modules we need. We really only require Pytorch and Pyro. Everything else is optional if you prefer to use your own data loader, visualization library, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "from torch.autograd import Variable\n",
    "from pyro.infer.kl_qp import KL_QP\n",
    "from pyro.distributions import DiagNormal, Normal\n",
    "from pyro.util import ng_zeros, ng_ones\n",
    "\n",
    "# modules from torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import visdom # (optional) for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load some data. We'll use [MNIST](http://yann.lecun.com/exdb/mnist/) for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to data\n",
    "root = './data'\n",
    "download = True\n",
    "trans = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    transform=trans,\n",
    "    download=download)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "# Use batch size of 128\n",
    "batch_size = 128\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "We first define our encoder, which takes the input image of size, and encodes it to a latent representation of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc21 = nn.Linear(200, 20)\n",
    "        self.fc22 = nn.Linear(200, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), torch.exp(self.fc22(h1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Next, our decoder will reproduce the original input from the latent represenation produced by the encoder in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-123324a4c118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc3 = nn.Linear(20, 200)\n",
    "        self.fc4 = nn.Linear(200, 2 * 784)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        rv = (self.fc4(h3))\n",
    "\n",
    "        # reshape to capture mu, sigma params for every pixel\n",
    "        rvs = rv.view(z.size(0), -1, 2)\n",
    "\n",
    "        # send back two params\n",
    "        return rvs[:, :, 0], torch.exp(rvs[:, :, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Now we want to define our model and guide to do inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "For this example, we are going to use variational inference to approximate the posterior by minimizing the [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) which maximizes the evidence lower bound (ELBO). In Pyro, this can by using `pyro.infer.ELBO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kl_optim = Elbo(model, guide, pyro.optim(optim.Adam, per_param_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the components we need for the VAE. To run the program, we just iterate over the number of epochs with our minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    for i in range(num_epochs):\n",
    "        epoch_loss = 0.\n",
    "        for ix, batch_start in enumerate(all_batches[:-1]):\n",
    "            batch_end = all_batches[ix + 1]\n",
    "            # get batch\n",
    "            batch_data = mnist_data[batch_start:batch_end]\n",
    "            epoch_loss += kl_optim.step(batch_data)\n",
    "        print(\"epoch avg loss {}\".format(epoch_loss / float(mnist_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert results and visualizations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data\n",
    "We use training data from MNIST, which consists of 55,000 $28\\times\n",
    "28$ pixel images (LeCun, Bottou, Bengio, & Haffner, 1998). Each image is represented\n",
    "as a flattened vector of 784 elements, and each element is a pixel\n",
    "intensity between 0 and 1.\n",
    "\n",
    "![GAN Fig 0](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/gan-fig0.png)\n",
    "\n",
    "\n",
    "The goal is to build and infer a model that can generate high quality\n",
    "images of handwritten digits.\n",
    "\n",
    "During training we will feed batches of MNIST digits. We instantiate a\n",
    "TensorFlow placeholder with a fixed batch size of $M$ images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "GANs posit generative models using an implicit mechanism. Given some\n",
    "random noise, the data is assumed to be generated by a deterministic\n",
    "function of that noise.\n",
    "\n",
    "Formally, the generative process is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\epsilon} &\\sim p(\\mathbf{\\epsilon}), \\\\\n",
    "\\mathbf{x} &= G(\\mathbf{\\epsilon}; \\theta),\n",
    "\\end{align*}\n",
    "\n",
    "where $G(\\cdot; \\theta)$ is a neural network that takes the samples\n",
    "$\\mathbf{\\epsilon}$ as input. The distribution\n",
    "$p(\\mathbf{\\epsilon})$ is interpreted as random noise injected to\n",
    "produce stochasticity in a physical system; it is typically a fixed\n",
    "uniform or normal distribution with some latent dimensionality.\n",
    "\n",
    "In Edward, we build the model as follows, using TensorFlow Slim to\n",
    "specify the neural network. It defines a 2-layer fully connected neural\n",
    "network and outputs a vector of length $28\\times28$ with values in\n",
    "$[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "A key idea in likelihood-free methods is to learn by\n",
    "comparison (e.g., Rubin (1984; Gretton, Borgwardt, Rasch, Schölkopf, & Smola, 2012)): by\n",
    "analyzing the discrepancy between samples from the model and samples\n",
    "from the true data distribution, we have information on where the\n",
    "model can be improved in order to generate better samples.\n",
    "\n",
    "In GANs, a neural network $D(\\cdot;\\phi)$ makes this comparison,\n",
    "known as the discriminator.\n",
    "$D(\\cdot;\\phi)$ takes data $\\mathbf{x}$ as input (either\n",
    "generations from the model or data points from the data set), and it\n",
    "calculates the probability that $\\mathbf{x}$ came from the true data.\n",
    "\\begin{equation*}\n",
    "\\min_\\theta \\max_\\phi~\n",
    "\\mathbb{E}_{p^*(\\mathbf{x})} [ \\log D(\\mathbf{x}; \\phi) ]\n",
    "+ \\mathbb{E}_{p(\\mathbf{x}; \\theta)} [ \\log (1 - D(\\mathbf{x}; \\phi)) ].\n",
    "\\end{equation*}\n",
    "\n",
    "In Edward, we use the following discriminative network. It is simply a\n",
    "feedforward network with one ReLU hidden layer. It returns the\n",
    "probability in the logit (unconstrained) scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use ADAM as optimizers for both the generator and discriminator.\n",
    "We'll run the algorithm for 15,000 iterations and print progress every\n",
    "1,000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now form the main loop which trains the GAN. At each iteration, it\n",
    "takes a minibatch and updates the parameters according to the\n",
    "algorithm. At every 1000 iterations, it will print progress and also\n",
    "saves a figure of generated samples from the model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
