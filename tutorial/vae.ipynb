{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "Variational autoencoders (VAE) are generative networks that incorporate neural networks and latent variable models.  It consists of an encoder and a decoder that seeks to reconstruct the input from a compressed latent represenation of the input.  It is a great example of how we can use variational inference to solve machine learning problems. We demonstrate how to easily implement a VAE in Pyro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import the packages and modules we need. We really only require Pytorch and Pyro. Everything else is optional if you prefer to use your own data loader, visualization library, etc. For this example, we will use [visdom](https://github.com/facebookresearch/visdom) to visualize the output images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "# import pyro!\n",
    "import pyro\n",
    "from pyro.infer.kl_qp import KL_QP\n",
    "from pyro.distributions import DiagNormal, Normal\n",
    "from pyro.util import ng_zeros, ng_ones\n",
    "\n",
    "# modules from torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import visdom # (optional) for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load some data. We'll use [MNIST](http://yann.lecun.com/exdb/mnist/) for simplicity, which contain images of size 28x28 flattened into a vector of length 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to data\n",
    "root = './data'\n",
    "download = True\n",
    "trans = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    transform=trans,\n",
    "    download=download)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "# Use batch size of 128\n",
    "batch_size = 128\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Our goal is to generate high quality images of handwritten digits. We first define our encoder, which is a neural net with 3 fully connected layers. It takes the input vector of size 784, and through non-linearities, encodes it to a latent representation with 20 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc21 = nn.Linear(200, 20)\n",
    "        self.fc22 = nn.Linear(200, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), torch.exp(self.fc22(h1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Next, our decoder will reproduce the original input from the latent represenation produced by the encoder in the previous step. It assumes an input of size 20 and through a series of non-linearities, outputs a vector of 2 * 784 (for $\\mu$ and $\\sigma$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-237887c4dcec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc3 = nn.Linear(20, 200)\n",
    "        self.fc4 = nn.Linear(200, 2 * 784)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        rv = (self.fc4(h3))\n",
    "\n",
    "        # reshape to capture mu, sigma params for every pixel\n",
    "        rvs = rv.view(z.size(0), -1, 2)\n",
    "\n",
    "        # send back two params\n",
    "        return rvs[:, :, 0], torch.exp(rvs[:, :, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Now we want to define our model and guide to do inference. The model samples from the prior $\\mathcal{N}(\\mu=0, \\sigma=1)$ and runs the decoder on the sample to calculate the parameters for the image distribution. We then score the image data against a Gaussian parameterized by the $\\mu, \\sigma$ generated by the decoder in the previous step. This is done via an `observe()` statement.\n",
    "\n",
    "The guide is the approximating distribution which will be used to sample when inference is run. It simply samples from a `DiagNormal()` parameterized by $\\mu$ and $\\sigma$ from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # klqp gets called with data.\n",
    "\n",
    "    # wrap params for use in model -- required\n",
    "    decoder = pyro.module(\"decoder\", pt_decode)\n",
    "    \n",
    "    # sample from prior\n",
    "    z_mu, z_sigma = ng_zeros(\n",
    "        [data.size(0), 20]), ng_ones([data.size(0), 20])\n",
    "\n",
    "    # sample (retrieve value set by the guide)\n",
    "    z = pyro.sample(\"latent\", DiagNormal(z_mu, z_sigma))\n",
    "\n",
    "    # decode into size of imgx2 for mu/sigma\n",
    "    img_mu, img_sigma = decoder.forward(z)\n",
    "\n",
    "    # score against actual images\n",
    "    pyro.observe(\"obs\", DiagNormal(img_mu, img_sigma), data.view(-1, 784))\n",
    "\n",
    "\n",
    "def guide(data):\n",
    "    # wrap params for use in model -- required\n",
    "    encoder = pyro.module(\"encoder\", pt_encode)\n",
    "\n",
    "    # use the ecnoder to get an estimate of mu, sigma\n",
    "    z_mu, z_sigma = encoder.forward(data)\n",
    "\n",
    "    pyro.sample(\"latent\", DiagNormal(z_mu, z_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "For this example, we are going to use variational inference to approximate the posterior by minimizing the [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the approximate latent distribution and the true posterior, which equivalently maximizes the evidence lower bound (ELBO).\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathrm{ELBO} = \n",
    "\\mathbb{E}_{q)} [ \\log p(\\mathbf{x}, \\mathbf{z};) ]\n",
    "- \\mathbb{E}_{q} [ \\log q(\\mathbf{z}) ].\n",
    "\\end{equation*}\n",
    "\n",
    "In Pyro, this can by using `pyro.infer.ELBO`. We'll use the [ADAM](https://arxiv.org/abs/1412.6980) algorithm as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KL_QP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1f64e6a84d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkl_optim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKL_QP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.0001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'KL_QP' is not defined"
     ]
    }
   ],
   "source": [
    "kl_optim = Elbo(model, guide, pyro.optim(optim.Adam, {\"lr\": .0001}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the components we need for the VAE. To run the program, we just iterate over the number of epochs with our minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    for i in range(num_epochs):\n",
    "        epoch_loss = 0.\n",
    "        for ix, batch_start in enumerate(all_batches[:-1]):\n",
    "            batch_end = all_batches[ix + 1]\n",
    "            # get batch\n",
    "            batch_data = mnist_data[batch_start:batch_end]\n",
    "            epoch_loss += kl_optim.step(batch_data)\n",
    "        print(\"epoch avg loss {}\".format(epoch_loss / float(mnist_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila! We visualize the results with `visdom`.\n",
    "\n",
    "Insert results and visualizations here\n",
    "![Fig](link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the full code on [Github](https://github.com/uber/pyro/blob/dev/examples/vae.py)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
