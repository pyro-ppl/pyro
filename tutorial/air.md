Current status: Our Pyro implementation of AIR is not working
satisfactorily yet. We have everything we need to describe the model
and inference strategy, but we don't yet achieve the results described
in the paper. Work on this is continuing.

# Attend Infer Repeat

In this tutorial we will implement the "Attend Infer Repeat" (AIR)
model[1] and apply it to the multi-mnist dataset.

## Intro

To get an idea where we're heading, we first give a brief overview of
the model and the approach we'll take to inference.

AIR is a generative model of images[2]. In this tutorial we will use
it to model images from the multi-mnist dataset. Here are some example
data points from this data set:

```
------ ------ ------ 
|2   | |    | |    | 
|    | |    | | 5  | 
|  7 | |  0 | |3   | 
------ ------ ------ 
```


AIR decomposes the process of generating a whole image into discrete
steps, each of which generates part of the whole image. We'll call
these parts "objects". In the case of AIR applied to the multi-mnist
dataset we expect each of these objects to represent a single digit.
The model includes uncertainty about the position and scale of each
object, and the number of objects within each image.

To model the variability of individual digits, the object generated at
each step is produced by passing a latent variable through a neural
network. The parameters of this network will be optimized during
inference.

Following the paper we will apply amortized variational inference to
this model. Performing inference in such rich models is always
difficult, but the presence of discrete choices (the number of steps
in this case) makes inference in this model particularly tricky. For
this reason the authors use a technique called data dependent
baselines to achieve good, and we'll see later in the tutorial how to
implement this in Pyro.



## The Model

The core of the model is the generative process for a single object.
Recall that:

* At each step a single object is generated.

* We maintain uncertainty about the latent variable used to generate
  each object, as well as its position & scale.

* Each object is generated by passing its latent variable through a
  neural network.

This can be expressed in Pyro like so:

```python
# Create the decoder network. This is a PyTorch module.
decode = Decoder()

def model_step(t):

    # Sample object position. This is a 3-vector
    describing the position and scale of the object.
    z_where = pyro.sample('z_where_{}'.format(t), DiagNormal())

    # Sample latent code for object.
    z_what = pyro.sample('z_what_{}'.format(t), DiagNormal())
    
    # Map latent code to pixel space using the neural network.
    y_att = decode(z_what)

    # Position/scale object within larger image.
    y = object_to_image(z_where, object_size, x_size, y_att)

    return y
```

(Note that this code is only a sketch of an implementation that we
will build up throughout the tutorial and is not executable. A
complete implementation that follows the tutorial closely is available
here. TODO: link)

Hopefully the use of `pyro.sample` and PyTorch networks within a model
seems familiar at this point. If not you might want to review earlier
parts of the tutorials.

The `object_to_image` function is specific to this model and warrants
further attention. Recall that the `decoder` network will output a
small (say 28x28 pixel) image patch, and that we would like to add
this to the output image after performing any scaling and translation
described by `z_where`.

It's not obvious that this can be implemented in a way that preserves
the differentiability of our model, but it turns out that it can,
using something called a spatial transformer network (STN)[3].

Happily for us, PyTorch includes a STN implementation that we can use
in our model. In fact, `object_to_image` simply wraps this
implementation, and does a little extra work to massage the sampled
`z_where` into the expected format. (TODO: link/code?)


That completes the implementation of the model for a single object, so
we now need to consider how we use this to generate an entire image.
Recall that would like to maintain uncertainty over the number of
steps used to generate each data point. To achieve this, we will
sample from a Bernoulli distribution at each step and continue making
further steps until we sample a `0`. By doing so, we specify a
geometric prior over the number of steps. We'll call the value sampled
from the distribution `z_pres`.


At this point we run into a snag. In our final implementation we would
like to generate an entire mini batch of samples in parallel for
efficiency.

However, at present Pyro assumes that within `iarange` the size of the
sampled mini batch is constant, more precisely all elements within the
mini batch must sample from the same set of random choices. For us
this is problematic because as we have just described, each sample
from our model can make a different number of choices.

One simple solution, and the one we'll take here, is to make a fixed
number of steps for each sample, but only add the generated object to
the final image when `z_pres=1`.

^^^^ START CHOP ^^^^

TODO: Consider dropping this log_pdf_mask bit. It's not clear this is
actually helpful in the model, so it might get pruned before we're
done anyway.

Going further, we can also "mask out" the choices made after the
indicator variable signals that a sample is complete.

This can be implemented using the `log_pdf_mask` argument of
`pyro.sample`. When we sample a batch of choices we will pass a binary
vector as `log_pdf_mask` that has a `0` at positions corresponding to
data points that have completed the generative process and `1`
otherwise. This has the effect of removing completed choices from the
model from the perspective of the inference back end, while allowing
us to perform the same computation for all data points, making it easy
to write code to generate an entire batch in parallel.

^^^^ END CHOP ^^^^


Even though this approach performs redundant computation, the gains
from using mini batches are so large that this is still a win overall.
(Eventually though, we'd like to be able to express the model in a way
that avoids this redundant computation.)

Putting this all together, the generative process for a single step
looks something like this:


```python
def model_step():

    # Sample presence indicators. The probability of generating
    # another object is 0 if previous sampled a zero indicating the data
    # point is done, and 0.5 otherwise.
    z_pres = pyro.sample('z_pres_{}'.format(t),
                         Bernoulli(0.5 * prev.z_pres))

    # If zero is sampled for a data point, then no more objects will
    # be added to the output. We can't straight-forwardly avoid
    # generating further objects, so instead we zero out the log_pdf
    # of future choices.
    sample_mask = z_pres

    # Sample object position, masking complete data points. 
    z_where = pyro.sample('z_where_{}'.format(t),
                          DiagNormal(self.z_where_mu_prior,
                                     self.z_where_sigma_prior,
                                     batch_size=n),
                          log_pdf_mask=sample_mask)

    # Sample latent code for the object window, masking complete data points.
    z_what = pyro.sample('z_what_{}'.format(t),
                         DiagNormal(self.ng_zeros([self.z_what_size]),
                                    self.ng_ones([self.z_what_size]),
                                    batch_size=n),
                                    log_pdf_mask=sample_mask)

    # Map latent code to pixel space.
    y_att = self.decode(z_what)

    # Position/scale object within larger image.
    y = objects_to_images(z_where, self.window_size, self.x_size, y_att)

    # Combine the object generated at this step with the image so far.
    # Only add the object if this data point is active.
    x = prev.x + (y * z_pres.view(-1, 1, 1))

```

In summary, we start with a blank image and iterate `model_step` three
times. Each step generates an object, and providing we don't sample
`z_pres=0` (which would indicate we're done) the generated object is
combined additively with the "image so far", `prev_x`


Each image in the multi-mnist dataset contains zero, one or two
digits, so we will allow the model to use up to (and including) three
steps. This will allow us to observe whether the model inferences
avoid using the unnecessary final step, and to test the model's
ability to generalize to images with more digits than were seeing
during optimization.




[TODO: Code:?]

The final step of the generative process adds a small amount of
Gaussian noise to each pixel.

[TOOD: Code?]

This completes the specification of the model. Let's vizualize some
samples from the model to get a feel for the prior this specifies:

[Show samples from the prior.]


The Guide
=========

We are going to perform amortized variational inference in this model.
(TODO: Read more about this here.) Pyro provides general purpose
machinery that implements most of this inference strategy, but as we
have seen in earlier tutorials we are required to provide a model
specific artefact that we call a "guide program".

The basic idea is that the guide program is in some sense the inverse
of the model. In our case, rather than sampling the latent space and
using the sample to generate an image, the guide will take an image as
input and from it generate a sample from the latent space. This guide
program will be endowed with tunable parameters, and the goal of
inference is then to adjust these parameters to make the distribution
of samples from the guide close to the posterior distribution of the
model.

The guide program we write for AIR will be constructed using neural
networks, so the tunable parameters of the guide will be the
parameters of the networks. What we call a guide program in Pyro is
exactly the entity called the "inference network" in the paper.

Let's think about how to structure a network that takes a multi-mnist
image as input, and outputs the parameters of the latents choices in
the model. First, recall that the model is split into a number of
discrete steps, therefore it's natural to structure the network as a
recurrent network. At each step the recurrent network will generate
the parameters for the choices made within the step.


^^^^ Ignore faffing with masks? ^^^^
Following the structure of the model, if a `0` is sampled from the
Bernoulli choice we will consider the image to be complete, and no
further steps will be performed. If a `1` is sampled, then we will
update the RNN hidden state with the sampled values and perform the
next step.

(We would like the guide to operate on a mini batch of inputs in
parallel, so as in the model, we will use a `log_pdf_mask` to mask out
the choices made for completed data points, in lieu of avoiding their
computation entirely.)


^^^^ Ignore faffing with masks? ^^^^

This can be implemented in Pyro like so:

[[TODO: add code. no attn window yet.]]


This would a reasonable guide to use with this model, but the paper
includes a crucial improvement we can make. Recall that the guide will
output information about an object's position and its latent code at
each step. The improvement we can make is based on the observation
that once we have computed the position of an object, we can do a
better job of computing its latent code if we use the position
information to crop the object from the image, and feed the result
into the network as an additional input.

(See figure 3b in the paper for a picture of this structure.)

Here's how we can implement this in Pyro:

[[TODO: add code]]

In practice we want to maintain differentiability of the guide, so we
will again use a spatial transformer to perform the cropping. The
`image_to_object` function performs the opposite transform to the
`object_to_image` function used in the guide. That is, the former
takes a patch and places it on a larger image, and the latter crops a
small patch from a larger image.


So far we've considered the model and the guide in isolation, but we
gain an interesting perspective if we zoom out and look at the model
and guide computation as a whole. Doing so, we see that at each time
step AIR includes a sub-computation that has the same structure as a
Variational Auto-encoder. (TODO: Link to VAE code/tutorial.)

To see this, notice that the guide passes the window through an
"encoder" network to generate the parameters of the distribution over
a latent code, and the model passes samples from this latent code
distribution through a "decoder" network to generate an output window.

From this perspective, the act of cropping the input image serves to
restrict the attention of the guide to a small region of the image at
each step. Hence "Attend, Infer, Repeat".

## Inference

As was mentioned in the introduction, successfully performing
inference in this model is a challenge. In particular, the presence of
discrete choices in the model makes inference trickier than in a model
in which all choices are continuous.

(TODO: Link to tutorial discussion of combined estimator, trickiness
of reinforce.)

The underlying problem we face is that the gradient estimates we use
in the optimization performed by variational inference have much
higher variance in the presence of discrete choices.

To bring this variance under control, the paper uses a technique
called "data dependent baselines". (AKA "neural baselines".)

## Baselines

To get a feel for what these are and how they work, we first need to
look a little more closely at how gradient estimates are made for
discrete choices.

For a discrete choice `q` with parameter `\phi`, Pyro uses a
Monte-Carlo estimate of the following expectation as the gradient of
the varitional objective:

```
grad L = E[grad log q(z_i) * log(q(z_i)/p(x_i,z_i))]
```

This is known in the literature as the REINFORCE or likelihood ratio
estimator.

The idea behind baselines depends on the fact that subtracting a
constant from `log(q()/p())` does not change the value of this
expectation. That is:

```
grad L = E[grad log q(z_i) * (log(q(z_i)/p(x_i,z_i)) - b_i)]
```

The key observation is that while subtracting such a constant from our
Monte-Carlo estimate will not change its expectation, it can change
its variance.

Of course, this immediately raises the question of what value we ought
to choose for `b_i` in order to reduce the variance of our estimates?

One simple choice (popularized by the reinforcement learning community
(TODO: right?)) is to take `b_i` to be the expectation of `log q/p`.
While this choice does not result in the maximum possible variance
reduction, (TODO: ref) it is simple to work with and works well in
practice.

Of course, we can't easily compute the exact value of *this*
expectation any more easily than we can compute the expectation on
which the gradient esimate is based. So again we will form a
Monte-Carlo estimate, using the same samples drawn to estimate the
gradient.

There's one final idea to introduce before we arrive at the final form
of baselines used in AIR. As described above, we have one `b_i` for
each data point in the data set. The final idea is to optimize a
neural network to output an estimate of `b_i` when given both the
image `x_i` and the value of the latent choices made so far as input.
The output of such a neural network is called a data dependent
baseline. [TODO: link to NVIL?]

The variational inference implementation in Pyro includes support for
data dependent baselines. As model authors we only have to implement
the neural network and pass its output to as an extra argument to a
discrete choice in the guide. The Pyro backend will take care of
automatically optimize the networks parameters (pushing it towards
outputting the baseline value described above) and will include the
baseline in the gradient estimator for the variational objective.

Now let's think about how we can add data dependent baselines to our
AIR implementation. We would like a neural network that can output a
baseline value at each discrete choice in the guide, having received
an input image and the value of any choices made so far as input.
Notice that this is very similar to what we required from the network
in the guide program, and in fact we can use a recurrent network with
the same structure here.

Here's the guide program for a single step, updated to include the
baseline network:

[TODO: code]

In practice there a few details to attend to in order to make this
work:

1. Notice that in the code we `detach` the values sampled for latent
choices before passing them to the baseline network. This is important
as the baseline network and the guide network are two separate
networks optimized with two different objectives. Without this,
gradients would flow from the baseline network in to the guide
network. We have to do this whenever we feed values sampled by the
guide into the baselines network. (TODO: Check, is there an error from
PyTorch if one doesn't?)

2. It can be useful to use different learning rates for the parameters
of these two networks. For AIR a learning rate of 1e-4 is used for the
guide network, and a learning rate of 1e-3 is used for the baseline
network. This is easy to implement in Pyro by tagging modules
associated with the baseline network and passing multiple learning
rates to the optimizer.



Putting It Altogether
=====================


We have now completed the implementation of the model and the guide.
To perform inference we only need to write a few more lines of code to
hand these over to Pyro and to specify which inference algorithm to
use.

[[TODO: Code]]

This says that we are performing stochastic variational inference
(SVI) and specifies the learning rates for the guide and baseline
network. The `trace_graph=True` option enables a more sophisticated
gradient estimator that is essential to achieving good results in the
presence of discrete choices. (TODO: See here for more info about
this.)


Results
=======

We'll add a results section once this implementation replicates the
results from the paper.




[1] AIR

[2] It's more general than this, but we'll focus on images in this
tutorial.

[3] STN
