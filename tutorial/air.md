Current status: Our Pyro implementation of AIR is not yet complete. We
think we have everything we need to describe the model and inference
strategy, but we don't yet achieve the results described in the paper.
Work on this is continuing.

# AIR

## Intro

In this tutorial we will implement the "Attend Infer Repeat" (AIR)
model[1] and apply it to the multi-mnist dataset.

To get an idea where we're heading, we'll first give a brief overview
of the model and the approach we'll take to inference.

AIR is a generative model of images[2]. In this tutorial we will use
it to model images from the multi-mnist dataset. Here are some example
data points from this data set:

```
------ ------ ------ 
|2   | |    | |    | 
|    | |    | | 5  | 
|  7 | |  0 | |3   | 
------ ------ ------ 
```


AIR decomposes the process of generating a whole image into discrete
steps, each of which generates part of the whole image. We'll call
these parts "objects". In the case of AIR applied to the multi-mnist
dataset we expect each of these objects to represent a single digit.
The model includes uncertainty about the position and scale of each
object, and the number of objects within each image.

To model the variability of individual digits, the object generated at
each step is produced by passing a latent variable through a neural
network. The parameters of this network will be optimized during
inference.

Following the paper we will apply amortized variational inference to
this model. Performing inference in such rich models is always a
difficult, but the presence of discrete choices (the number of steps)
makes inference in this model especially so. The paper describes a
number of techniques that can help to address this specific challenge,
and we'll see later in the tutorial how these can be implemented in
Pyro.




## The Model

The core of the model is the generative process for a single object.
Recall that:

* At each step a single object is added to the output image.

* We maintain uncertainty about the latent variable used to generate
  each object, as well as its position & scale.

* Each object is generated by passing its latent variable through a
  neural network.

This can be expressed in Pyro like so:

```python

class Decoder(nn.Module):
    def __init__():
        pass

    def forward(self, z):
        pass 

decode = Decoder()

def model_step(prev_x):
    # Sample object position.
    z_where = pyro.sample('z_where_{}'.format(t),
                          DiagNormal(z_where_mu_prior,
                                     z_where_sigma_prior))

    # Sample latent code for object.
    z_what = pyro.sample('z_what_{}'.format(t),
                         DiagNormal(ng_zeros([z_what_size]),
                                    ng_ones([z_what_size])))
    
    # Map latent code to pixel space using neural network.
    y_att = decode(z_what)

    # Position/scale object within larger image.
    y = objects_to_images(z_where, object_size, x_size, y_att)

    # Combine the object generated at this step with the image so far.
    x = prev_x + y
```

(Note that this code is only a sketch of an implementation that we
will build up throughout the tutorial. A more complete, and fully
functioning implementation that follows the tutorial closely is
available here.)

The `objects_to_images` function scales and translates the object
using a spatial transformer[3]. It returns an image which has the same
shape as the final output. The output of this step is combined
additively with "image so far" (`prev_x`) generated by previous steps.

We can iterate this `model_step` method several times to build up a
complete image, but recall that would also like to maintain
uncertainty over the number of steps used to generate each data point.
To achieve this, we can sample from a Bernoulli distribution at each
step and continue making further steps until we sample a `0`. This
specifies a geometric prior over the number of steps.

At this point we run into a snag. In our final implementation we would
like to generate an entire mini batch of data points in parallel for
efficiency. In our setting, doing so is complicated by the fact that
each sample can potentially take a different number of steps. (TODO:
Say why is this a problem.) One simple solution, and the one we'll
take here, is to make a fixed number of steps for each sample and to
"mask out" the choices made after the indicator variable signals that
a data point is complete.


This is implemented using the `log_pdf_mask` argument of
`pyro.sample`. When we sample a batch of choices we will pass a binary
vector as `log_pdf_mask` that has a `0` at positions corresponding to
data points that have completed the generative process and `1`
otherwise. This has the effect of removing completed choices from the
model from the perspective of the inference back end, while allowing
us to perform the same computation for all data points, making it easy
to write code to generate an entire batch in parallel.

Even though this approach performs redundant computation, the gains
from using mini batches are so large that this is still a win overall.
(Eventually though, we'd like to be able to express the model in a way
that avoids this redundant computation.)

Putting this all together, the generative process for a single step
looks something like this:


```python
def model_step():

    # Sample presence indicators. The probability of generating
    # another object is 0 if previous sampled a zero indicating the data
    # point is done, and 0.5 otherwise.
    z_pres = pyro.sample('z_pres_{}'.format(t),
                         Bernoulli(0.5 * prev.z_pres))

    # If zero is sampled for a data point, then no more objects will
    # be added to the output. We can't straight-forwardly avoid
    # generating further objects, so instead we zero out the log_pdf
    # of future choices.
    sample_mask = z_pres

    # Sample object position, masking complete data points. 
    z_where = pyro.sample('z_where_{}'.format(t),
                          DiagNormal(self.z_where_mu_prior,
                                     self.z_where_sigma_prior,
                                     batch_size=n),
                          log_pdf_mask=sample_mask)

    # Sample latent code for the object window, masking complete data points.
    z_what = pyro.sample('z_what_{}'.format(t),
                         DiagNormal(self.ng_zeros([self.z_what_size]),
                                    self.ng_ones([self.z_what_size]),
                                    batch_size=n),
                                    log_pdf_mask=sample_mask)

    # Map latent code to pixel space.
    y_att = self.decode(z_what)

    # Position/scale object within larger image.
    y = objects_to_images(z_where, self.window_size, self.x_size, y_att)

    # Combine the object generated at this step with the image so far.
    # Only add the object if this data point is active.
    x = prev.x + (y * z_pres.view(-1, 1, 1))

```

Each image in the multi-mnist dataset contains zero, one or two
digits. We will allow the model to use up to (and including) three
steps. This will allow us to observe whether the model inferences
avoid using the unnecessary final step, and to test the model's
ability to generalize to images with more digits than were seeing
during optimization.

By starting with a blank image and iterating `model_step` three times
we will progressively build a final image from a number of objects.

[TODO: Code:?]

The final step of the generative process adds a small amount of
Gaussian noise to each pixel.

[TOOD: Code?]

This completed the specification of the mode. We can now call our
`model` method to generate samples from the prior. These can be
visualised using some helper functions included with the example:

[Show samples from the prior.]


The Guide
=========

We're going to perform amortized variational inference in this model.
(TODO: Read more about this here.) Pyro provides general purpose
machinery that implements most of this inference strategy, but as
model authors we are required to provide a model specific artefact
that we call a "guide program".

The basic idea is that the guide program is in some sense the inverse
of the model. In our case, rather than sampling the latent space and
using the sample to generate an image, the guide will take an image as
input and from it generate a sample from the latent space. This guide
program will be endowed with tunable parameters, and the goal of
inference is then to adjust these parameters to make the distribution
of samples from the guide close to the posterior distribution of the
model.

The guide program we write for AIR will be constructed using neural
networks, so the tunable parameters of the guide will be the
parameters of the networks. What we call a guide program in Pyro is
exactly the entity called the "inference network" in the paper.

Let's think about how to structure a network that takes a multi-mnist
image as input, and outputs the parameters of the latents choices in
the model. First, recall that the model is split into a variable
number of discrete steps, so it's natural to structure the network as
a recurrent network. At each step the recurrent network will generate
the parameters for the choices made within the step, and these choices
will then be sampled. Following the structure of the model, if a `0`
is sampled from the Bernoulli choice we will consider the image to be
complete, and no further steps will be performed. If a `1` is sampled,
then we will update the RNN hidden state with the sampled values and
perform the next step.

(See figure 3b in the paper for a picture of this structure.)

This can be implemented in Pyro like so:

[[TODO: add code]]

(We would like the guide to operate on a mini batch of inputs in
parallel, so as in the model, we will use a `log_pdf_mask` to mask out
the choices made for completed data points, in lieu of avoiding their
computation entirely.)

The paper describes an important improvement we can make over the
basic network described above. Recall that we are going to output
information about an object's latent code and position/scale at each
step. The improvement is based on the observation that once we have
computed the position of an object, we can do a better job of
computing its latent code by using the position and scale information
to crop the object from the image, and feed the result into the
network as an additional input.

In practice we want to maintain differentiability of the guide, so we
will again use a spatial transformer to perform the cropping.

[[TODO: add code]]

So far we've considered the model and the guide in isolation, but we
gain an interesting perspective if we zoom out and look at the model
and guide computation as a whole. Doing so, we see that at each time
step AIR includes a sub-computation that has the same structure as a
Variational Auto-encoder. (TODO: Link to VAE code/tutorial.)

To see this, notice that the guide passes the window through an
"encoder" network to generate the parameters of the distribution over
a latent code, and the model passes samples from this latent code
distribution through a "decoder" network to generate an output window.

From this perspective, the act of cropping the input image serves to
restrict the attention of the guide to a small region of the image at
each step.

## Inference

As was mentioned in the introduction, successfully performing
inference in this model is a challenge. In particular, the presence of
discrete choices in the model makes inference trickier than in a model
in which all choices are continuous.

(TODO: Link to tutorial discussion of combined estimator, trickiness
of reinforce.)

The underlying problem we face is that the gradient estimates we use
in the optimization performed by variational inference have much
higher variance in the presence of discrete choices.

To bring this variance under control, the paper uses a technique
called "data dependent baselines". (AKA "neural baselines".)

## Baselines

To get a feel for what these are and how they work, we first need to
look a little more closely at how gradient estimates are made for
discrete choices.

For a discrete choice `q` with parameter `\phi`, Pyro uses a
Monte-Carlo estimate of the following expectation as the gradient of
the varitional objective:

```
grad L = E[grad log q(z_i) * log(q(z_i)/p(x_i,z_i))]
```

This is known in the literature as the REINFORCE or likelihood ratio
estimator.

The idea behind baselines depends on the fact that subtracting a
constant from `log(q()/p())` does not change the value of this
expectation. That is:

```
grad L = E[grad log q(z_i) * (log(q(z_i)/p(x_i,z_i)) - b_i)]
```

The key observation is that while subtracting such a constant from our
Monte-Carlo estimate will not change its expectation, it can change
its variance.

Of course, this immediately raises the question of what value we ought
to choose for `b_i` in order to reduce the variance of our estimates?

One simple choice (popularized by the reinforcement learning community
(TODO: right?)) is to take `b_i` to be the expectation of `log q/p`.
While this choice does not result in the maximum possible variance
reduction, (TODO: ref) it is simple to work with and works well in
practice.

Of course, we can't easily compute the exact value of *this*
expectation any more easily than we can compute the expectation on
which the gradient esimate is based. So again we will form a
Monte-Carlo estimate, using the same samples drawn to estimate the
gradient.

There's one final idea to introduce before we arrive at the final form
of baselines used in AIR. As described above, we have one `b_i` for
each data point in the data set. The final idea is to optimize a
neural network to output an estimate of `b_i` when given both the
image `x_i` and the value of the latent choices made so far as input.
The output of such a neural network is called a data dependent
baseline. [TODO: link to NVIL?]

The variational inference implementation in Pyro includes support for
data dependent baselines. As model authors we only have to implement
the neural network and pass its output to as an extra argument to a
discrete choice in the guide. The Pyro backend will take care of
automatically optimize the networks parameters (pushing it towards
outputting the baseline value described above) and will include the
baseline in the gradient estimator for the variational objective.

Now let's think about how we can add data dependent baselines to our
AIR implementation. We would like a neural network that can output a
baseline value at each discrete choice in the guide, having received
an input image and the value of any choices made so far as input.
Notice that this is very similar to what we required from the network
in the guide program, and in fact we can use a recurrent network with
the same structure here.

Here's the guide program for a single step, updated to include the
baseline network:

[TODO: code]

In practice there a few details to attend to in order to make this
work:

1. Notice that in the code we `detach` the values sampled for latent
choices before passing them to the baseline network. This is important
as the baseline network and the guide network are two separate
networks optimized with two different objectives. Without this,
gradients would flow from the baseline network in to the guide
network. We have to do this whenever we feed values sampled by the
guide into the baselines network. (TODO: Check, is there an error from
PyTorch if one doesn't?)

2. It can be useful to use different learning rates for the parameters
of these two networks. For AIR a learning rate of 1e-4 is used for the
guide network, and a learning rate of 1e-3 is used for the baseline
network. This is easy to implement in Pyro by tagging modules
associated with the baseline network and passing multiple learning
rates to the optimizer.



Putting It Altogether
=====================


We have now completed the implementation of the model and the guide.
To perform inference only need to write a few more lines of code to
hand these over to Pyro, and to specify which inference algorithm to
use.

[[TODO: Code]]

This says that we are performing stochastic variational inference
(SVI) and specifies the learning rates for the guide and baseline
network. The `trace_graph=True` option enables a more sophisticated
gradient estimator that is essential to achieving good results in the
presence of discrete choices. (TODO: See here for more info about
this.)


Results
=======

We'll add a results section once this implementation replicates the
results from the paper.




[1] AIR

[2] It's more general than this, but we'll focus on images in this
tutorial.

[3] A spatial transformer is a *differentialble* means of performing
this operation, making it suitable for use in with gradient based
optimization.
