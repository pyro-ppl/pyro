{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression\n",
    "\n",
    "Regression is one of the most common and basic supervised learning tasks in machine learning. It is used to fit a function to data. Linear regression generally takes the form:\n",
    "\\begin{equation}\n",
    "y = \\beta_1 x + \\beta_0 + \\sigma\n",
    "\\end{equation}\n",
    "where we would like to learn $\\beta_0$ and $\\beta_1$. Let's first write a normal regression as you would in PyTorch and learn point estimates for the parameters.  Then we'll see how to learn uncertainty by doing a bayesian regression over the same parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "As always, let's begin by importing the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize  # noqa: F401\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pyro\n",
    "from pyro import random_module\n",
    "from pyro.distributions import DiagNormal, Bernoulli  # noqa: F401\n",
    "from pyro.infer import SVI\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We'll generate a linear toy dataset with $\\beta_1 = 3$ and $\\beta_0 = 1$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_linear_dataset(N, noise_std=0.1):\n",
    "    X = np.linspace(-6, 6, num=N)\n",
    "    y = 3 * X + 1 + np.random.normal(0, noise_std, size=N)\n",
    "    X = X.reshape((N, 1))\n",
    "    y = y.reshape((N, 1))\n",
    "    X, y = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))\n",
    "    return torch.cat((X, y), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Now let's define our regression model in the form of a neural net. We'll use PyTorch's nn module for this.  Our input is a data of size $N \\times p$ and our output is a vector of size $p \\times 1$. `nn.Linear(1, 1)` defines a linear module of the form $Xw + b$ where $w$ is the weight matrix and $b$ is the additive bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "regression_model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We will use MSE as our loss and Adam as our optimizer. We would like to optimize the parameters of the `regression` neural net above. Since our toy dataset does not have a lot of noise, we will use a larger learning rate of `0.01` and run for 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11556.578125\n",
      "5533.54248047\n",
      "2333.60498047\n",
      "869.701904297\n",
      "312.388305664\n",
      "139.756820679\n",
      "96.7058105469\n",
      "88.0949783325\n",
      "86.7161560059\n",
      "86.5399780273\n",
      "[('linear.weight', Parameter containing:\n",
      " 2.9794\n",
      "[torch.FloatTensor of size 1x1]\n",
      "), ('linear.bias', Parameter containing:\n",
      " 1.0688\n",
      "[torch.FloatTensor of size 1]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "N = 100  # size of toy data\n",
    "p = 1  # number of features\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optim = torch.optim.Adam(regression_model.parameters(), lr=0.01)\n",
    "num_epochs = 1000\n",
    "\n",
    "def main():\n",
    "    data = build_linear_dataset(N, p)\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    for j in range(num_epochs):\n",
    "        y_pred = regression_model(x_data)\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if j % 100 == 0:\n",
    "            print loss.data[0]\n",
    "    # Inspect learned parameters\n",
    "    print list(regression_model.named_parameters())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad - you can see that the neural net learned parameters that were pretty close to the ground truth $w = 3, b = 1$.  However, what if our data was noisy? How confident are we that the learned parameters reflect the true values?\n",
    "\n",
    "This is a fundamental limitation of deep learning that we can address with probabilistic modeling.  Instead of only learning the point estimates, we learn a _distribution_ over the possible parameters.  In other words, we'll learn two values for each parameter: $\\mu$ which is the mean (ie the actual value) and $\\sigma$, our uncertainty for that estimate.\n",
    "\n",
    "## Bayesian Regression\n",
    "Instead of learning these parameters directly, we'll put a prior over these parameters, and learn a posterior distribution given our observed data.  To do this, we'll use pyro's `random_module()` to lift the parameters we would like to learn.  `random_module()` replaces the original parameters of the neural net with random variables sampled from our prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = Variable(torch.zeros(1, 1))\n",
    "sigma = Variable(torch.ones(1, 1))\n",
    "# define a prior we want to sample from\n",
    "prior = DiagNormal(mu, sigma)\n",
    "# overload the parameters in the regression nn with samples from the prior\n",
    "lifted_module = pyro.random_module(\"regression_module\", regression_model, prior)\n",
    "# sample a nn from the prior\n",
    "sampled_nn = lifted_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Now we want to define our model and guide to do inference. The model samples from the prior $\\mathcal{N}(\\mu=0, \\sigma=1)$ and runs the decoder on the sample to calculate the parameters for the image distribution. We then score the image data against a Gaussian parameterized by the $\\mu, \\sigma$ generated by the decoder in the previous step. This is done via an `observe()` statement.\n",
    "\n",
    "The guide is the approximating distribution which will be used to sample when inference is run. It simply samples from a `DiagNormal()` parameterized by $\\mu$ and $\\sigma$ from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # Create unit normal priors over the parameters\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    mu = Variable(torch.zeros(p, 1))\n",
    "    sigma = Variable(torch.ones(p, 1))\n",
    "    bias_mu = Variable(torch.zeros(1))\n",
    "    bias_sigma = Variable(torch.ones(1))\n",
    "    w_prior = DiagNormal(mu, sigma)\n",
    "    b_prior = DiagNormal(bias_mu, bias_sigma)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "    # wrap regression model that lifts module parameters to random variables\n",
    "    # sampled from the priors in the guide\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    # sample a nn\n",
    "    lifted_nn = lifted_module()\n",
    "    latent = lifted_nn(x_data).squeeze()\n",
    "    pyro.observe(\"obs\", DiagNormal(latent, Variable(torch.ones(data.size(0)))), y_data.squeeze())\n",
    "\n",
    "\n",
    "def guide(data):\n",
    "    w_mu = Variable(torch.randn(p, 1), requires_grad=True)\n",
    "    w_log_sig = Variable(-3.0 * torch.ones(p, 1) + 0.05 * torch.randn(p, 1), requires_grad=True)\n",
    "    b_mu = Variable(torch.randn(1), requires_grad=True)\n",
    "    b_log_sig = Variable(-3.0 * torch.ones(1) + 0.05 * torch.randn(1), requires_grad=True)\n",
    "    # register learnable params in the param store\n",
    "    mw_param = pyro.param(\"guide_mean_weight\", w_mu)\n",
    "    sw_param = softplus(pyro.param(\"guide_sigma_weight\", w_log_sig))\n",
    "    mb_param = pyro.param(\"guide_mean_bias\", b_mu)\n",
    "    sb_param = softplus(pyro.param(\"guide_sigma_bias\", b_log_sig))\n",
    "    # gaussian priors for w and b\n",
    "    w_prior, b_prior = DiagNormal(mw_param, sw_param), DiagNormal(mb_param, sb_param)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "    # overloading the parameters in the module with random samples from the prior\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    # sample a nn\n",
    "    lifted_module()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Pyro makes it easy to do variational inference with the objective and estimator of your choice. We'll still use the ADAM optimizer with a learning rate of 0.01, but this time we'ere going to optimize the evidence lower bound (ELBO).  For more information on the ELBO see [LINK].  To train, we will iterate over the number of epochs and feed the data to our SVI object. We'll print the loss every 100 epochs, and plot the ELBO during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch avg loss 92.6841503906\n",
      "epoch avg loss 56.4399707031\n",
      "epoch avg loss 29.8963354492\n",
      "epoch avg loss 16.2691894531\n",
      "epoch avg loss 7.24183654785\n",
      "epoch avg loss 3.38534301758\n",
      "epoch avg loss 2.19528579712\n",
      "epoch avg loss 1.83481796265\n",
      "epoch avg loss 1.54607284546\n",
      "epoch avg loss 1.51113754272\n"
     ]
    }
   ],
   "source": [
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=\"ELBO\")\n",
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def main():\n",
    "    data = build_linear_dataset(N, p)\n",
    "    for j in range(num_epochs):\n",
    "        # calculate the loss\n",
    "        epoch_loss = svi.step(data)\n",
    "        if j % 100 == 0:\n",
    "            print(\"epoch avg loss {}\".format(epoch_loss/float(N)))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Criticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our output to our previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guide_sigma_bias': Variable containing:\n",
      "-2.2457\n",
      "[torch.FloatTensor of size 1]\n",
      ", 'guide_sigma_weight': Variable containing:\n",
      "-3.1321\n",
      "[torch.FloatTensor of size 1x1]\n",
      ", 'guide_mean_weight': Variable containing:\n",
      " 2.9890\n",
      "[torch.FloatTensor of size 1x1]\n",
      ", 'guide_mean_bias': Variable containing:\n",
      " 1.0727\n",
      "[torch.FloatTensor of size 1]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print pyro.get_param_store()._params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the means are pretty close to the value we previously learned; however, instead of a point estimate, we learned a _distribution over possible values_ of $w, b$. (Note that the $\\sigma$s are actually $\\log \\sigma$ so the more negative the value is, the narrower the width.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model by checking its predicting accuracy on new test data. This is known as _posterior predictive checks_.  We'll calculate the MSE of our synthesized data compared to the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  1.0779\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.linspace(8, 12, num=20)\n",
    "y = 3 * X + 1\n",
    "X, y = X.reshape((20, 1)), y.reshape((20, 1))\n",
    "x_data, y_data = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))\n",
    "y_pred = regression_model(x_data)\n",
    "loss = nn.MSELoss()\n",
    "print loss(y_pred, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the full code on [Github](https://github.com/uber/pyro/blob/dev/examples/bayesian_regression.py)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
