{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression\n",
    "\n",
    "Regression is one of the most common and basic supervised learning tasks in machine learning. It is used to fit a function to observed data. Linear regression generally takes the form:\n",
    "\\begin{equation}\n",
    "y = \\beta_1 X + \\beta_0 + \\epsilon\n",
    "\\end{equation}\n",
    "where we would like to learn $\\beta_0$ and $\\beta_1$. Let's first write a normal regression as you would in PyTorch and learn point estimates for the parameters.  Then we'll see how to learn uncertainty by doing bayesian inference over the same parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "As always, let's begin by importing the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We'll generate a linear toy dataset with one feature and $\\beta_1 = 3$ and $\\beta_0 = 1$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100  # size of toy data\n",
    "p = 1  # number of features\n",
    "\n",
    "def build_linear_dataset(N, noise_std=0.1):\n",
    "    X = np.linspace(-6, 6, num=N)\n",
    "    y = 3 * X + 1 + np.random.normal(0, noise_std, size=N)\n",
    "    X, y = X.reshape((N, 1)), y.reshape((N, 1))\n",
    "    X, y = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))\n",
    "    return torch.cat((X, y), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Now let's define our regression model in the form of a neural network. We'll use PyTorch's `nn.Module` for this.  Our input $X$ is a data of size $N \\times p$ and our output $y$ is a vector of size $p \\times 1$.  The function `nn.Linear(p, 1)` defines a linear module of the form $Xw + b$ where $w$ is the weight matrix and $b$ is the additive bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(p, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "regression_model = RegressionModel(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We will use MSE as our loss and Adam as our optimizer. We would like to optimize the parameters of the `regression_model` neural net above. Since our toy dataset does not have a lot of noise, we will use a larger learning rate of `0.01` and run for 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optim = torch.optim.Adam(regression_model.parameters(), lr=0.01)\n",
    "num_epochs = 1000\n",
    "\n",
    "def main():\n",
    "    data = build_linear_dataset(N, p)\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    for j in range(num_epochs):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model(x_data)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize zero gradients\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if j % 100 == 0:\n",
    "            print loss.data[0]\n",
    "    # Inspect learned parameters\n",
    "    print \"Parameters:\", list(regression_model.named_parameters())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad - you can see that the neural net learned parameters that were pretty close to the ground truth $w = 3, b = 1$.  However, what if our data was noisy? How confident are we that the learned parameters reflect the true values?\n",
    "\n",
    "This is a fundamental limitation of deep learning that we can address with probabilistic modeling.  Instead of only learning the point estimates, we learn a _distribution_ over the possible parameters.  In other words, we'll learn two values for each parameter: $\\mu$ which is the mean (ie the actual value) and $\\sigma$, our uncertainty for that estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Regression\n",
    "Instead of learning these parameters directly, we'll put a prior over these parameters, and learn a posterior distribution given our observed data.  To do this, we'll use pyro's `random_module()` to lift the parameters we would like to learn.  `random_module()` replaces the original parameters of the neural net with random variables sampled from our prior.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = Variable(torch.zeros(1, 1))\n",
    "sigma = Variable(torch.ones(1, 1))\n",
    "# define a prior we want to sample from\n",
    "prior = Normal(mu, sigma)\n",
    "# overload the parameters in the regression nn with samples from the prior\n",
    "lifted_module = pyro.random_module(\"regression_module\", regression_model, prior)\n",
    "# sample a nn from the prior\n",
    "sampled_nn = lifted_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Our model defines unit Gaussians for both the weight and the bias, samples a nn from the prior defined in the guide, and runs the nn forward on the data. We then score this predicted value against the observed value, with a fixed variance.\n",
    "\n",
    "The guide defines priors over the weights and biases.  The parameters we want to learn are registered in the param store via `pyro.param()`.  Note that we pass the log variances through a `softplus()` to ensure positivity. We then define Gaussian priors with these parameters and wrap the `regression_model` with `pyro.random_module()`. `lifted_module` is a distribution over nns and calling the function samples a nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # Create unit normal priors over the parameters\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    mu, sigma = Variable(torch.zeros(p, 1)), Variable(torch.ones(p, 1))\n",
    "    bias_mu, bias_sigma = Variable(torch.zeros(1)), Variable(torch.ones(1))\n",
    "    w_prior, b_prior = Normal(mu, sigma), Normal(bias_mu, bias_sigma)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "    # wrap regression model that lifts module parameters to random variables\n",
    "    # sampled from the priors in the guide\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    # sample a nn\n",
    "    lifted_nn = lifted_module()\n",
    "    # run the nn forward\n",
    "    latent = lifted_nn(x_data).squeeze()\n",
    "    # condition on the observed data\n",
    "    pyro.observe(\"obs\", Normal(latent, Variable(torch.ones(data.size(0)))), y_data.squeeze())\n",
    "\n",
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(data):\n",
    "    w_mu = Variable(torch.randn(p, 1), requires_grad=True)\n",
    "    w_log_sig = Variable(-3.0 * torch.ones(p, 1) + 0.05 * torch.randn(p, 1), requires_grad=True)\n",
    "    b_mu = Variable(torch.randn(1), requires_grad=True)\n",
    "    b_log_sig = Variable(-3.0 * torch.ones(1) + 0.05 * torch.randn(1), requires_grad=True)\n",
    "    # register learnable params in the param store\n",
    "    mw_param = pyro.param(\"guide_mean_weight\", w_mu)\n",
    "    sw_param = softplus(pyro.param(\"guide_log_sigma_weight\", w_log_sig))\n",
    "    mb_param = pyro.param(\"guide_mean_bias\", b_mu)\n",
    "    sb_param = softplus(pyro.param(\"guide_log_sigma_bias\", b_log_sig))\n",
    "    # gaussian priors for w and b\n",
    "    w_prior, b_prior = Normal(mw_param, sw_param), Normal(mb_param, sb_param)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "    # overloading the parameters in the module with random samples from the prior\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    # sample a nn\n",
    "    lifted_module()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "For inference, we'll still use the Adam optimizer with a learning rate of 0.01, but this time we're going to optimize the evidence lower bound (ELBO).  For more information on the ELBO and SVI, see the [SVI Tutorial](svi_part_i).  To train, we will iterate over the number of epochs and feed the data to our SVI object. We'll print the loss every 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=\"ELBO\")\n",
    "\n",
    "def main():\n",
    "    data = build_linear_dataset(N, p)\n",
    "    for j in range(num_epochs):\n",
    "        # calculate the loss and take a gradient step\n",
    "        epoch_loss = svi.step(data)\n",
    "        if j % 100 == 0:\n",
    "            print(\"epoch avg loss {}\".format(epoch_loss/float(N)))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Criticism\n",
    "Let's compare our output to our previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print pyro.get_param_store()._params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the means are pretty close to the value we previously learned; however, instead of a point estimate, we learned a _distribution over possible values_ of $w, b$. (Note that we are using $\\log \\sigma$, so the more negative the value is, the narrower the width.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model by checking its predicting accuracy on new test data. This is known as _point evaluation_.  We'll calculate the MSE of our synthesized data compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(8, 12, num=20)\n",
    "y = 3 * X + 1\n",
    "X, y = X.reshape((20, 1)), y.reshape((20, 1))\n",
    "x_data, y_data = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))\n",
    "y_pred = regression_model(x_data)\n",
    "loss = nn.MSELoss()\n",
    "# compare the MSE between the observed data and the data predicted by our posterior\n",
    "print loss(y_pred, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the full code on [Github](https://github.com/uber/pyro/blob/dev/examples/bayesian_regression.py)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
