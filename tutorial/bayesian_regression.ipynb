{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression\n",
    "\n",
    "Regression is one of the most common and basic supervised learning tasks in machine learning. It is used to fit a function to data. Linear regression generally takes the form:\n",
    "\\begin{equation}\n",
    "y = \\beta_1 x + \\beta_0 + \\sigma\n",
    "\\end{equation}\n",
    "where we would like to learn $\\beta_0$ and $\\beta_1$. Let's first write a normal regression as you would in PyTorch and learning point estimates for the parameters.  Then we'll see how to learn uncertainty by doing a bayesian regression over the same parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "As always, let's begin by importing the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize  # noqa: F401\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pyro\n",
    "from pyro import random_module\n",
    "from pyro.distributions import DiagNormal, Bernoulli  # noqa: F401\n",
    "from pyro.infer import SVI\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We'll generate a linear toy dataset with $\\beta_1 = 3$ and $\\beta_0 = 1$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_linear_dataset(N, noise_std=0.1):\n",
    "    X = np.linspace(-6, 6, num=N)\n",
    "    y = 3 * X + 1 + np.random.normal(0, noise_std, size=N)\n",
    "    X = X.reshape((N, 1))\n",
    "    y = y.reshape((N, 1))\n",
    "    X, y = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))\n",
    "    return torch.cat((X, y), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our regression model in the form of a neural net. We'll use PyTorch's nn module for this.  Our input is a data of size $N \\times p$ and our output is a vector of size $p \\times 1$. `nn.Linear(1, 1)` defines a linear module of the form $Xw + b$ where $w$ is the weight matrix and $b$ is the additive bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regression, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "regression = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We will use MSE as our loss and Adam as our optimizer. We would like to optimize the parameters of the `regression` neural net above. Since our toy dataset does not have a lot of noise, we will use a larger learning rate of `0.01` and run for 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.420830369\n",
      "0.917385041714\n",
      "0.917374432087\n",
      "0.917373895645\n",
      "0.91737383604\n",
      "0.91737395525\n",
      "0.917375266552\n",
      "0.917375266552\n",
      "0.91737395525\n",
      "0.91737383604\n",
      "[('linear.weight', Parameter containing:\n",
      " 3.0021\n",
      "[torch.FloatTensor of size 1x1]\n",
      "), ('linear.bias', Parameter containing:\n",
      " 1.0078\n",
      "[torch.FloatTensor of size 1]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "N = 100  # size of toy data\n",
    "p = 1  # number of features\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(regression.parameters(), lr=0.01)\n",
    "num_epochs = 1000\n",
    "\n",
    "def main():\n",
    "    x, y = build_linear_dataset(N, p)\n",
    "    for j in range(num_epochs):\n",
    "        y_pred = regression(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 100 == 0:\n",
    "            print loss.data[0]\n",
    "    # Inspect learned parameters\n",
    "    print list(regression.named_parameters())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad - you can see that the neural net learned parameters that were pretty close to the ground truth $w = 3, b = 1$.  However, what if our data was noisy? How confident are we that the learned parameters reflect the true values?\n",
    "\n",
    "This is a fundamental limitation of deep learning that we can address with probabilistic modeling.  Instead of only learning the point estimates, we learn a _distribution_ over the possible parameters.  In other words, we'll learn two values for each parameter: $\\mu$ which is the mean (ie the actual value) and $\\sigma$, our uncertainty for that estimate.\n",
    "\n",
    "Instead of learning these parameters directly, we'll put a prior over these parameters, and learn a posterior distribution given our observed data.  To do this, we'll use pyro's `random_module()` to lift the parameters we would like to learn.  `random_module()` replaces the original parameters of the neural net with random variables sampled from our prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Regression (\n",
       "  (linear): Linear (1 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = Variable(torch.zeros(1, 1))\n",
    "sigma = Variable(torch.ones(1, 1))\n",
    "# define a prior we want to sample from\n",
    "prior = DiagNormal(mu, sigma)\n",
    "# overload the parameters in the regression nn with samples from the prior\n",
    "lifted_module = pyro.random_module(\"regression_module\", regression, prior)\n",
    "# sample a nn from the prior\n",
    "sampled_nn = lifted_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Now we want to define our model and guide to do inference. The model samples from the prior $\\mathcal{N}(\\mu=0, \\sigma=1)$ and runs the decoder on the sample to calculate the parameters for the image distribution. We then score the image data against a Gaussian parameterized by the $\\mu, \\sigma$ generated by the decoder in the previous step. This is done via an `observe()` statement.\n",
    "\n",
    "The guide is the approximating distribution which will be used to sample when inference is run. It simply samples from a `DiagNormal()` parameterized by $\\mu$ and $\\sigma$ from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # klqp gets called with data.\n",
    "\n",
    "    # wrap params for use in model -- required\n",
    "    decoder = pyro.module(\"decoder\", pt_decode)\n",
    "    \n",
    "    # sample from prior\n",
    "    z_mu, z_sigma = ng_zeros(\n",
    "        [data.size(0), 20]), ng_ones([data.size(0), 20])\n",
    "\n",
    "    # sample (retrieve value set by the guide)\n",
    "    z = pyro.sample(\"latent\", DiagNormal(z_mu, z_sigma))\n",
    "\n",
    "    # decode into size of imgx2 for mu/sigma\n",
    "    img_mu, img_sigma = decoder.forward(z)\n",
    "\n",
    "    # score against actual images\n",
    "    pyro.observe(\"obs\", DiagNormal(img_mu, img_sigma), data.view(-1, 784))\n",
    "\n",
    "\n",
    "def guide(data):\n",
    "    # wrap params for use in model -- required\n",
    "    encoder = pyro.module(\"encoder\", pt_encode)\n",
    "\n",
    "    # use the ecnoder to get an estimate of mu, sigma\n",
    "    z_mu, z_sigma = encoder.forward(data)\n",
    "\n",
    "    pyro.sample(\"latent\", DiagNormal(z_mu, z_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Pyro makes it easy to do variational inference with the objective and estimator of your choice. For this exaample, we'll optimize the ELBO using the ADAM optimizer with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ad458b3f0d60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0madam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ELBO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "adam = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, adam, loss=\"ELBO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "To train, we will iterate over the number of epochs and feed the data to our SVI object. We'll print the loss every 100 epochs, and plot the ELBO during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-98067c168d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;31m# mini batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the results by inspecting the parameters of the learned approximate posterior. Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">> print pyro.get_param_store()._params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the full code on [Github](https://github.com/uber/pyro/blob/dev/examples/bayesian_regression.py)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
