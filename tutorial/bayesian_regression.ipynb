{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression\n",
    "\n",
    "Regression is one of the most common and basic machine learning tasks. It is used to fit a function to data. Let's first write a normal regression as you would in PyTorch.  Let's define our model and guide to do inference. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "# import pyro!\n",
    "import pyro\n",
    "from pyro.infer.kl_qp import KL_QP\n",
    "from pyro.distributions import DiagNormal, Normal\n",
    "from pyro.util import ng_zeros, ng_ones\n",
    "\n",
    "# modules from torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import visdom # (optional) for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put on our bayesian hats and try to learn these parameters in a bayesian way! To do this, we'll use pyro's `random_module()` to lift the parameters we would like to learn.  Instead of learning these parameters directly, we'll put a prior over these parameters, and learn a posterior distribution given our observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Next, our decoder will reproduce the original input from the latent represenation produced by the encoder in the previous step. It assumes an input of size 20 and through a series of non-linearities, outputs a vector of 2 * 784 (for $\\mu$ and $\\sigma$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-237887c4dcec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc3 = nn.Linear(20, 200)\n",
    "        self.fc4 = nn.Linear(200, 2 * 784)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        rv = (self.fc4(h3))\n",
    "\n",
    "        # reshape to capture mu, sigma params for every pixel\n",
    "        rvs = rv.view(z.size(0), -1, 2)\n",
    "\n",
    "        # send back two params\n",
    "        return rvs[:, :, 0], torch.exp(rvs[:, :, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Now we want to define our model and guide to do inference. The model samples from the prior $\\mathcal{N}(\\mu=0, \\sigma=1)$ and runs the decoder on the sample to calculate the parameters for the image distribution. We then score the image data against a Gaussian parameterized by the $\\mu, \\sigma$ generated by the decoder in the previous step. This is done via an `observe()` statement.\n",
    "\n",
    "The guide is the approximating distribution which will be used to sample when inference is run. It simply samples from a `DiagNormal()` parameterized by $\\mu$ and $\\sigma$ from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # klqp gets called with data.\n",
    "\n",
    "    # wrap params for use in model -- required\n",
    "    decoder = pyro.module(\"decoder\", pt_decode)\n",
    "    \n",
    "    # sample from prior\n",
    "    z_mu, z_sigma = ng_zeros(\n",
    "        [data.size(0), 20]), ng_ones([data.size(0), 20])\n",
    "\n",
    "    # sample (retrieve value set by the guide)\n",
    "    z = pyro.sample(\"latent\", DiagNormal(z_mu, z_sigma))\n",
    "\n",
    "    # decode into size of imgx2 for mu/sigma\n",
    "    img_mu, img_sigma = decoder.forward(z)\n",
    "\n",
    "    # score against actual images\n",
    "    pyro.observe(\"obs\", DiagNormal(img_mu, img_sigma), data.view(-1, 784))\n",
    "\n",
    "\n",
    "def guide(data):\n",
    "    # wrap params for use in model -- required\n",
    "    encoder = pyro.module(\"encoder\", pt_encode)\n",
    "\n",
    "    # use the ecnoder to get an estimate of mu, sigma\n",
    "    z_mu, z_sigma = encoder.forward(data)\n",
    "\n",
    "    pyro.sample(\"latent\", DiagNormal(z_mu, z_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "For this example, we are going to use variational inference to approximate the posterior by minimizing the [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the approximate latent distribution and the true posterior, which equivalently maximizes the evidence lower bound (ELBO).\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathrm{ELBO} = \n",
    "\\mathbb{E}_{q)} [ \\log p(\\mathbf{x}, \\mathbf{z};) ]\n",
    "- \\mathbb{E}_{q} [ \\log q(\\mathbf{z}) ].\n",
    "\\end{equation*}\n",
    "\n",
    "In Pyro, this can by using `pyro.infer.ELBO`. We'll use the [ADAM](https://arxiv.org/abs/1412.6980) algorithm as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KL_QP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1f64e6a84d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkl_optim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKL_QP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.0001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'KL_QP' is not defined"
     ]
    }
   ],
   "source": [
    "kl_optim = Elbo(model, guide, pyro.optim(optim.Adam, {\"lr\": .0001}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the components we need for the VAE. To run the program, we just iterate over the number of epochs with our minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    for i in range(num_epochs):\n",
    "        epoch_loss = 0.\n",
    "        for ix, batch_start in enumerate(all_batches[:-1]):\n",
    "            batch_end = all_batches[ix + 1]\n",
    "            # get batch\n",
    "            batch_data = mnist_data[batch_start:batch_end]\n",
    "            epoch_loss += kl_optim.step(batch_data)\n",
    "        print(\"epoch avg loss {}\".format(epoch_loss / float(mnist_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila! We visualize the results with `visdom`.\n",
    "\n",
    "Insert results and visualizations here\n",
    "![Fig](link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the full code on [Github](https://github.com/uber/pyro/blob/dev/examples/vae.py)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
